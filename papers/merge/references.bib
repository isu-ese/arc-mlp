
@article{durand_efficient_1999,
	title = {An efficient algorithm for similarity analysis of molecules},
	volume = {2},
	number = {17},
	journal = {Internet Journal of Chemistry},
	author = {Durand, Paul J. and Pasari, Rohit and Baker, Johnnie W. and Tsai, Chun-che},
	year = {1999},
	pages = {1--16},
	file = {Full Text:/home/grifisaa/Zotero/storage/EP988BBA/paper.html:text/html}
}

@inproceedings{welling_performance_2011,
	title = {A {Performance} {Analysis} on {Maximal} {Common} {Subgraph} {Algorithms}},
	abstract = {Graphs can be used as a tool to determine similarity between structured objects. The maximal common subgraph of two graphs G and H is the largest graph in terms of edges that is isomorphic to a subgraph of G and H. Finding the maximal common subgraph is an NP-complete problem. It is useful in many areas like (bio)chemistry, file versioning and artificial intelligence. There are many papers that evaluate algorithms for finding maximal common induced subgraphs, but little research has been done on the maximal common subgraph that is not an induced subgraph. We have implemented and benchmarked two maximal common (not induced) subgraph algorithms: a backtrack search algorithm (McGregor), and an algorithm that transforms the maximal common subgraph problem to the largest clique problem (Koch). We created generators for randomly connected and mesh structured graphs, these generators have been used to create a database of graph pairs to benchmark the two algorithms. The results of our benchmark have shown that in most cases Koch is more efficient, because after creating the edge product graph needed for the clique detection. The actual clique detection is a relatively simple search.},
	author = {Welling, Ruud},
	year = {2011},
	keywords = {Artificial intelligence, Backtracking, Benchmark (computing), Clique (graph theory), Clique problem, Graph - visual representation, Induced subgraph, Koch snowflake, Maximal set, Paper, Randomness, Search algorithm},
	file = {Full Text PDF:/home/grifisaa/Zotero/storage/W6DZM6J7/Welling - 2011 - A Performance Analysis on Maximal Common Subgraph .pdf:application/pdf}
}

@article{conte_challenging_2007,
	title = {Challenging {Complexity} of {Maximum} {Common} {Subgraph} {Detection} {Algorithms}: {A} {Performance} {Analysis} of {Three} {Algorithms} on a {Wide} {Database} of {Graphs}},
	volume = {11},
	shorttitle = {Challenging {Complexity} of {Maximum} {Common} {Subgraph} {Detection} {Algorithms}},
	doi = {10.7155/jgaa.00139},
	abstract = {Graphs are an extremely general and powerful data structure. In pattern recognition and computer vision, graphs are used to represent patterns to be recognized or classified. Detection of maximum common subgraph (MCS) is useful for matching, comparing and evaluate the similarity of patterns. MCS is a well known NP-complete problem for which optimal and suboptimal algorithms are known from the literature. Nevertheless, until now no effort has been done for characterizing their performance. The lack of a large database of graphs makes the task of comparing the performance of different graph matching algorithms difficult, and often the selection of an algorithm is made on the basis of a few experimental results available. In this paper, three optimal and well-known algorithms for maximum common subgraph detection are described. Moreover a large database containing various categories of pairs of graphs (e.g. random graphs, meshes, bounded valence graphs), is presented, and the performance of the three algorithms is evaluated on this database. Article Type Communicated by Submitted Revised Regular Paper U. Brandes September 2005 January 2007 D. Conte et al., Maximum Common Subgraph, JGAA, 11(1) 99–143 (2007) 100},
	journal = {J. Graph Algorithms Appl.},
	author = {Conte, Donatello and Foggia, Pasquale and Vento, Mario},
	year = {2007},
	keywords = {Algorithm, Computer vision, Data structure, Graph (discrete mathematics), Journal of Graph Algorithms and Applications, Matching (graph theory), NP-completeness, Pattern recognition, Random graph},
	pages = {99--143},
	file = {Full Text PDF:/home/grifisaa/Zotero/storage/J5G7JFXE/Conte et al. - 2007 - Challenging Complexity of Maximum Common Subgraph .pdf:application/pdf}
}

@inproceedings{conte_comparison_2003,
	address = {Berlin, Heidelberg},
	series = {{GbRPR}'03},
	title = {A {Comparison} of {Three} {Maximum} {Common} {Subgraph} {Algorithms} on a {Large} {Database} of {Labeled} {Graphs}},
	isbn = {978-3-540-40452-1},
	url = {http://dl.acm.org/citation.cfm?id=1757868.1757884},
	abstract = {A graph g is called a maximum common subgraph of two graphs, g1 and g2, if there exists no other common subgraph of g1 and g2 that has more nodes than g. For the maximum common subgraph problem, exact and inexact algorithms are known from the literature. Nevertheless, until now no effort has been done for characterizing their performance, mainly for the lack of a large database of graphs. In this paper, three exact and well-known algorithms for maximum common subgraph detection are described. Moreover, a large database containing various categories of pairs of graphs (e.g. randomly connected graphs, meshes, bounded valence graphs...), having a maximum common subgraph of at least two nodes, is presented, and the performance of the three algorithms is evaluated on this database.},
	urldate = {2019-05-10},
	booktitle = {Proceedings of the 4th {IAPR} {International} {Conference} on {Graph} {Based} {Representations} in {Pattern} {Recognition}},
	publisher = {Springer-Verlag},
	author = {Conte, D. and Guidobaldi, C. and Sansone, C.},
	year = {2003},
	note = {event-place: York, UK},
	pages = {130--141},
	file = {Conte et al. - 2003 - A Comparison of Three Maximum Common Subgraph Algo.pdf:/home/grifisaa/Zotero/storage/NM4V4NS3/Conte et al. - 2003 - A Comparison of Three Maximum Common Subgraph Algo.pdf:application/pdf}
}

@article{raymond_maximum_nodate,
	title = {Maximum common subgraph isomorphism algorithms for the matching of chemical structures},
	abstract = {The maximum common subgraph (MCS) problem has become increasingly important in those aspects of chemoinformatics that involve the matching of 2D or 3D chemical structures. This paper provides a classiﬁcation and a review of the many MCS algorithms, both exact and approximate, that have been described in the literature, and makes recommendations regarding their applicability to typical chemoinformatics tasks.},
	language = {en},
	author = {Raymond, John W and Willett, Peter},
	pages = {13},
	file = {Raymond and Willett - Maximum common subgraph isomorphism algorithms for.pdf:/home/grifisaa/Zotero/storage/JFM6B5X6/Raymond and Willett - Maximum common subgraph isomorphism algorithms for.pdf:application/pdf}
}

@article{tomita_worst-case_2006,
	series = {Computing and {Combinatorics}},
	title = {The worst-case time complexity for generating all maximal cliques and computational experiments},
	volume = {363},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/S0304397506003586},
	doi = {10.1016/j.tcs.2006.06.015},
	abstract = {We present a depth-first search algorithm for generating all maximal cliques of an undirected graph, in which pruning methods are employed as in the Bron–Kerbosch algorithm. All the maximal cliques generated are output in a tree-like form. Subsequently, we prove that its worst-case time complexity is O(3n/3) for an n-vertex graph. This is optimal as a function of n, since there exist up to 3n/3 maximal cliques in an n-vertex graph. The algorithm is also demonstrated to run very fast in practice by computational experiments.},
	number = {1},
	urldate = {2019-05-09},
	journal = {Theoretical Computer Science},
	author = {Tomita, Etsuji and Tanaka, Akira and Takahashi, Haruhisa},
	month = oct,
	year = {2006},
	keywords = {Computational experiments, Enumeration, Maximal cliques, Worst-case time complexity},
	pages = {28--42},
	file = {ScienceDirect Full Text PDF:/home/grifisaa/Zotero/storage/GD8CK37I/Tomita et al. - 2006 - The worst-case time complexity for generating all .pdf:application/pdf;ScienceDirect Snapshot:/home/grifisaa/Zotero/storage/DSFQQ4WR/S0304397506003586.html:text/html}
}

@inproceedings{bettenburg_what_2008,
	title = {What makes a good bug report?},
	booktitle = {Proceedings of the 16th {ACM} {SIGSOFT} {International} {Symposium} on {Foundations} of software engineering},
	publisher = {ACM},
	author = {Bettenburg, Nicolas and Just, Sascha and Schröter, Adrian and Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
	year = {2008},
	pages = {308--318},
	file = {Full Text:/home/grifisaa/Zotero/storage/69P6IZPU/Bettenburg et al. - 2008 - What makes a good bug report.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/VW9QKCMZ/citation.html:text/html}
}

@inproceedings{bastani_synthesizing_2017,
	address = {Barcelona, Spain},
	title = {Synthesizing program input grammars},
	isbn = {978-1-4503-4988-8},
	url = {http://dl.acm.org/citation.cfm?doid=3062341.3062349},
	doi = {10.1145/3062341.3062349},
	abstract = {We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.},
	language = {en},
	urldate = {2019-04-18},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} {Conference} on {Programming} {Language} {Design} and {Implementation}  - {PLDI} 2017},
	publisher = {ACM Press},
	author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
	year = {2017},
	pages = {95--110},
	file = {Bastani et al. - 2017 - Synthesizing program input grammars.pdf:/home/grifisaa/Zotero/storage/RTMDXFT8/Bastani et al. - 2017 - Synthesizing program input grammars.pdf:application/pdf}
}

@article{klint_toward_2005,
	title = {Toward an engineering discipline for grammarware},
	volume = {14},
	issn = {1049331X},
	url = {http://portal.acm.org/citation.cfm?doid=1072997.1073000},
	doi = {10.1145/1072997.1073000},
	language = {en},
	number = {3},
	urldate = {2019-04-18},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Klint, Paul and Lämmel, Ralf and Verhoef, Chris},
	month = jul,
	year = {2005},
	pages = {331--380},
	file = {Klint et al. - 2005 - Toward an engineering discipline for grammarware.pdf:/home/grifisaa/Zotero/storage/R5CQWU7B/Klint et al. - 2005 - Toward an engineering discipline for grammarware.pdf:application/pdf}
}

@article{goloveshkin_tolerant_2018,
	title = {Tolerant parsing with a special kind of «{Any}» symbol: the algorithm and practical application},
	volume = {30},
	issn = {20798156, 22206426},
	shorttitle = {Tolerant parsing with a special kind of «{Any}» symbol},
	url = {http://www.ispras.ru/en/proceedings/isp_30_2018_4/isp_30_2018_4_7/},
	doi = {10.15514/ISPRAS-2018-30(4)-1},
	abstract = {Tolerant parsing is a form of syntax analysis aimed at capturing the structure of certain points of interest presented in a source code. While these points should be welldescribed in the corresponding language grammar, other parts of the program are allowed to be not presented in the grammar or to be described coarse-grained, thereby parser remains tolerant to the possible inconsistencies in the irrelevant area. Island grammars are one of the basic tolerant parsing techniques. “Island” is used as the relevant code alias, while the irrelevant code is called “water”. In the paper, a modified LL(1) parsing algorithm with built-in “Any” symbol processing is described. The “Any” symbol matches implicitly defined token sequences. The use of the algorithm for island grammars allows one to reduce irrelevant code description as well as to simplify patterns for relevant code matching. Our “Any” implementation is more accurate and less restrictive in comparison with the closest analogues implemented in Coco/R and LightParse parser generators. It also has potentially lower overhead than the “bounded seas” concept implemented in PetitParser. As shown in the experimental section, the tolerant parser generated by the C\# island grammar is proven to be applicable for large-scale software projects analysis.},
	language = {en},
	number = {4},
	urldate = {2019-04-19},
	journal = {Proceedings of the Institute for System Programming of the RAS},
	author = {Goloveshkin, A.V. and Mikhalkovich, S.S.},
	year = {2018},
	pages = {7--28},
	file = {SFU, Rostov-on-Don, Russia et al. - 2018 - Tolerant parsing with a special kind of «Any» symb.pdf:/home/grifisaa/Zotero/storage/BL72465M/SFU, Rostov-on-Don, Russia et al. - 2018 - Tolerant parsing with a special kind of «Any» symb.pdf:application/pdf}
}

@article{shang_taming_2008,
	title = {Taming verification hardness: an efficient algorithm for testing subgraph isomorphism},
	volume = {1},
	shorttitle = {Taming verification hardness},
	number = {1},
	journal = {Proceedings of the VLDB Endowment},
	author = {Shang, Haichuan and Zhang, Ying and Lin, Xuemin and Yu, Jeffrey Xu},
	year = {2008},
	pages = {364--375},
	file = {Full Text:/home/grifisaa/Zotero/storage/Q3LF2EF4/Shang et al. - 2008 - Taming verification hardness an efficient algorit.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/3MG7BSLN/citation.html:text/html}
}

@inproceedings{synytskyy_robust_2003,
	title = {Robust multilingual parsing using island grammars},
	booktitle = {Proceedings of the 2003 conference of the {Centre} for {Advanced} {Studies} on {Collaborative} research},
	publisher = {IBM Press},
	author = {Synytskyy, Nikita and Cordy, James R. and Dean, Thomas R.},
	year = {2003},
	pages = {266--278},
	file = {Full Text:/home/grifisaa/Zotero/storage/XHC7TP7D/Synytskyy et al. - 2003 - Robust multilingual parsing using island grammars.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/45F3DFMF/citation.html:text/html}
}

@article{grindley_identification_1993,
	title = {Identification of tertiary structure resemblance in proteins using a maximal common subgraph isomorphism algorithm},
	volume = {229},
	number = {3},
	journal = {Journal of molecular biology},
	author = {Grindley, Helen M. and Artymiuk, Peter J. and Rice, David W. and Willett, Peter},
	year = {1993},
	pages = {707--721},
	file = {Snapshot:/home/grifisaa/Zotero/storage/C4SEFYTN/S0022283683710740.html:text/html}
}

@inproceedings{bacchelli_extracting_2011,
	title = {Extracting structured data from natural language documents with island parsing},
	booktitle = {Automated {Software} {Engineering} ({ASE}), 2011 26th {IEEE}/{ACM} {International} {Conference} on},
	publisher = {IEEE},
	author = {Bacchelli, Alberto and Cleve, Anthony and Lanza, Michele and Mocci, Andrea},
	year = {2011},
	pages = {476--479},
	file = {Full Text:/home/grifisaa/Zotero/storage/PMUGZSXY/Bacchelli et al. - 2011 - Extracting structured data from natural language d.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/W9XP9HUD/6100103.html:text/html}
}

@inproceedings{strein_cross-language_2006,
	title = {Cross-language program analysis and refactoring},
	booktitle = {2006 {Sixth} {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}},
	publisher = {IEEE},
	author = {Strein, Dennis and Kratz, Hans and Lowe, Welf},
	year = {2006},
	pages = {207--216},
	file = {Full Text:/home/grifisaa/Zotero/storage/RBCU5HUQ/Strein et al. - 2006 - Cross-language program analysis and refactoring.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/NFQSNL6F/4026870.html:text/html}
}

@book{reinhard_wilhelm_compiler_1995,
	address = {Boston, United States},
	title = {Compiler {Design}},
	isbn = {0-201-42290-5},
	publisher = {Addison-Wesley},
	author = {Reinhard Wilhelm, Dieter Maurer},
	month = jan,
	year = {1995}
}

@inproceedings{deursen_building_1999,
	title = {Building documentation generators},
	doi = {10.1109/ICSM.1999.792497},
	abstract = {In order to maintain the consistency between sources and documentation, while at the same time providing documentation at the design level, it is necessary to generate documentation from sources in such a way that it can be integrated with hand-written documentation. In order to simplify the construction of documentation generators, we introduce island grammars, which only define those syntactic structures needed for (re)documentation purposes. We explain how they can be used to obtain various forms of documentation, such as data dependency diagrams for mainframe batch jobs. Moreover, we discuss how the derived information can be made available via a hypertext structure. We conclude with an industrial case study in which a 600,000 LOC COBOL legacy system is redocumented using the techniques presented in the paper.},
	booktitle = {Proceedings {IEEE} {International} {Conference} on {Software} {Maintenance} - 1999 ({ICSM}'99). '{Software} {Maintenance} for {Business} {Change}' ({Cat}. {No}.99CB36360)},
	author = {Deursen, A. Van and Kuipers, T.},
	month = aug,
	year = {1999},
	keywords = {Cost function, system documentation, Read only memory, 600,000 LOC COBOL legacy system, data dependency diagrams, design level, Documentation, documentation generators, grammars, hand-written documentation, hypermedia, hypertext structure, island grammars, Lab-on-a-chip, mainframe batch jobs, Outsourcing, redocumentation, syntactic structures},
	pages = {40--49},
	file = {IEEE Xplore Abstract Record:/home/grifisaa/Zotero/storage/YER64GRN/792497.html:text/html;Submitted Version:/home/grifisaa/Zotero/storage/R2BMQH8K/Deursen and Kuipers - 1999 - Building documentation generators.pdf:application/pdf}
}

@article{ullmann_algorithm_1976,
	title = {An algorithm for subgraph isomorphism},
	volume = {23},
	number = {1},
	journal = {Journal of the ACM (JACM)},
	author = {Ullmann, Julian R.},
	year = {1976},
	pages = {31--42},
	file = {Full Text:/home/grifisaa/Zotero/storage/NLZT4HNG/Ullmann - 1976 - An algorithm for subgraph isomorphism.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/XFGZ3G6Q/citation.html:text/html}
}

@article{messmer_new_1998,
	title = {A new algorithm for error-tolerant subgraph isomorphism detection},
	volume = {20},
	number = {5},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Messmer, Bruno T. and Bunke, Horst},
	year = {1998},
	pages = {493--504},
	file = {IEEE Xplore Abstract Record:/home/grifisaa/Zotero/storage/B532ITRU/682179.html:text/html;IEEE Xplore Full Text PDF:/home/grifisaa/Zotero/storage/TK9VC4P2/Messmer and Bunke - 1998 - A new algorithm for error-tolerant subgraph isomor.pdf:application/pdf}
}

@inproceedings{bunke_comparison_2002,
	title = {A comparison of algorithms for maximum common subgraph on randomly connected graphs},
	booktitle = {Joint {IAPR} {International} {Workshops} on {Statistical} {Techniques} in {Pattern} {Recognition} ({SPR}) and {Structural} and {Syntactic} {Pattern} {Recognition} ({SSPR})},
	publisher = {Springer},
	author = {Bunke, Horst and Foggia, Pasquale and Guidobaldi, Corrado and Sansone, Carlo and Vento, Mario},
	year = {2002},
	pages = {123--132},
	file = {Full Text:/home/grifisaa/Zotero/storage/CUCLS6S4/Bunke et al. - 2002 - A comparison of algorithms for maximum common subg.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/HA46DUSM/10.html:text/html}
}

@article{cordella_sub_2004,
	title = {A (sub) graph isomorphism algorithm for matching large graphs},
	volume = {26},
	number = {10},
	journal = {IEEE transactions on pattern analysis and machine intelligence},
	author = {Cordella, Luigi P. and Foggia, Pasquale and Sansone, Carlo and Vento, Mario},
	year = {2004},
	pages = {1367--1372},
	file = {Full Text:/home/grifisaa/Zotero/storage/IVLLX2RR/Cordella et al. - 2004 - A (sub) graph isomorphism algorithm for matching l.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/K2NFRBUD/1323804.html:text/html}
}

@incollection{eppstein_subgraph_2002,
	title = {Subgraph isomorphism in planar graphs and related problems},
	booktitle = {Graph {Algorithms} {And} {Applications} {I}},
	publisher = {World Scientific},
	author = {Eppstein, David},
	year = {2002},
	pages = {283--309},
	file = {Full Text:/home/grifisaa/Zotero/storage/HD5NN6I4/Eppstein - 2002 - Subgraph isomorphism in planar graphs and related .pdf:application/pdf}
}

@inproceedings{moonen_lightweight_2002,
	title = {Lightweight {Impact} {Analysis} using {Island} {Grammars}.},
	booktitle = {{IWPC}},
	publisher = {Citeseer},
	author = {Moonen, Leon},
	year = {2002},
	pages = {219--228},
	file = {Full Text:/home/grifisaa/Zotero/storage/RTH5Q3U9/Moonen - 2002 - Lightweight Impact Analysis using Island Grammars..pdf:application/pdf}
}

@book{haoxiang_languages_1988,
	address = {Boston, MA, USA},
	edition = {3rd},
	title = {Languages and {Machines} {An} {Introduction} to the {Theory} of {Computer} {Science}},
	isbn = {0-201-15768-3},
	abstract = {Preface The objective of the third edition of Languages and Machines: An Introduction to the Theory of Computer Science remains the same as that of the first two editions, to provide a mathematically sound presentation of the theory of computer},
	language = {en},
	publisher = {Addison-Wesley Longman Publishing Co. Inc.},
	author = {Haoxiang, Ma},
	year = {1988},
	file = {Snapshot:/home/grifisaa/Zotero/storage/43FAYXNN/Languages_and_Machines_An_Introduction_to_the_Theory_of_Computer_Science.html:text/html}
}

@inproceedings{moonen_generating_2001,
	title = {Generating robust parsers using island grammars},
	doi = {10.1109/WCRE.2001.957806},
	abstract = {Source model extraction, the automated extraction of information from system artifacts, is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages). The paper proposes a solution in the form of island grammars, a special kind of grammar that combines the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of MANGROVE, a generator for source model extractors based on island grammars and describe its application to a number of case studies.},
	booktitle = {Proceedings {Eighth} {Working} {Conference} on {Reverse} {Engineering}},
	author = {Moonen, L.},
	month = oct,
	year = {2001},
	keywords = {Software maintenance, reverse engineering, program compilers, Application software, Maintenance engineering, Robustness, grammars, island grammars, automated information extraction, case studies, computational linguistics, Computer languages, Data mining, detailed specification, embedded languages, fuzzy parsing, incomplete source code, language dialects, lexical approaches, Libraries, MANGROVE, Mars, parser generation, partial parsing, program analysis, Reverse engineering, reverse engineering domain, reverse engineering tools, robust parser generation, robust parsers, source model extraction, source model extractors, syntactic errors, syntactical analysis, system artifacts, Transaction databases},
	pages = {13--22},
	file = {IEEE Xplore Abstract Record:/home/grifisaa/Zotero/storage/F767EEMY/957806.html:text/html;IEEE Xplore Full Text PDF:/home/grifisaa/Zotero/storage/HVVDIHDJ/Moonen - 2001 - Generating robust parsers using island grammars.pdf:application/pdf}
}

@book{ghezzi_fundamentals_2002,
	address = {Upper Saddle River, NJ, USA},
	edition = {2nd},
	title = {Fundamentals of {Software} {Engineering}},
	isbn = {978-0-13-305699-0},
	abstract = {From the Publisher:This book provides selective, in-depth coverage of the fundamentals of software engineering by stressing principles and methods through rigorous formal and informal approaches. In contrast to other books which are based on the lifecycle model of software development, the authors emphasize identifying and applying fundamental principles that are applicable throughout the software lifecycle. This emphasis enables readers to respond to the rapid changes in technology that are common today. Principles and techniques are emphasized rather than specific tools—users learn why particular techniques should or should not be used. Understanding the principles and techniques on which tools are based makes mastering a variety of specific tools easier. The authors discuss principles such as design, specification, verification, production, management and tools. Now coverage includes: more detailed analysis and explanation of object-oriented techniques; the use of Unified Modeling Language (UML); requirements analysis and software architecture; Model checking—a technique that provides automatic support to the human activity of software verification; GQM—used to evaluate software quality and help improve the software process; Z specification language. For software engineers.},
	publisher = {Prentice Hall PTR},
	author = {Ghezzi, Carlo and Jazayeri, Mehdi and Mandrioli, Dino},
	year = {2002}
}

@inproceedings{kuramochi_frequent_2001,
	title = {Frequent subgraph discovery},
	booktitle = {Data {Mining}, 2001. {ICDM} 2001, {Proceedings} {IEEE} international conference on},
	publisher = {IEEE},
	author = {Kuramochi, Michihiro and Karypis, George},
	year = {2001},
	pages = {313--320},
	file = {Full Text:/home/grifisaa/Zotero/storage/FRLU9FTD/Kuramochi and Karypis - 2001 - Frequent subgraph discovery.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/TULKIHWA/989534.html:text/html}
}

@inproceedings{huan_efficient_2003,
	title = {Efficient mining of frequent subgraphs in the presence of isomorphism},
	booktitle = {null},
	publisher = {IEEE},
	author = {Huan, Jun and Wang, Wei and Prins, Jan},
	year = {2003},
	pages = {549},
	file = {Full Text:/home/grifisaa/Zotero/storage/WINPSKMB/Huan et al. - 2003 - Efficient mining of frequent subgraphs in the pres.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/L4YQ8NQI/19780549.html:text/html}
}

@inproceedings{klusener_deriving_2003,
	title = {Deriving tolerant grammars from a base-line grammar},
	booktitle = {Software {Maintenance}, 2003. {ICSM} 2003. {Proceedings}. {International} {Conference} on},
	publisher = {IEEE},
	author = {Klusener, Steven and Lammel, Ralf},
	year = {2003},
	pages = {179--188},
	file = {Full Text:/home/grifisaa/Zotero/storage/UFACT5SD/Klusener and Lammel - 2003 - Deriving tolerant grammars from a base-line gramma.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/NY4WTEIW/1235420.html:text/html}
}

@article{kurs_bounded_2015,
	title = {Bounded seas},
	volume = {44},
	journal = {Computer languages, systems \& structures},
	author = {Kurš, Jan and Lungu, Mircea and Iyadurai, Rathesan and Nierstrasz, Oscar},
	year = {2015},
	pages = {114--140},
	file = {Full Text:/home/grifisaa/Zotero/storage/RTWVIN2X/Kurš et al. - 2015 - Bounded seas.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/2MGNZCVB/S1477842415000536.html:text/html}
}

@inproceedings{collard_xml-based_2003,
	title = {An {XML}-based lightweight {C}++ fact extractor},
	booktitle = {Program {Comprehension}, 2003. 11th {IEEE} {International} {Workshop} on},
	publisher = {IEEE},
	author = {Collard, Michael L. and Kagdi, Huzefa H. and Maletic, Jonathan I.},
	year = {2003},
	pages = {134--143},
	file = {Full Text:/home/grifisaa/Zotero/storage/VY4I8RQI/Collard et al. - 2003 - An XML-based lightweight C++ fact extractor.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/2V6ZIF2D/1199197.html:text/html}
}

@inproceedings{carroll_island_1983,
	title = {An island parsing interpreter for the full augmented transition network formalism},
	booktitle = {Proceedings of the first conference on {European} chapter of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Carroll, John A.},
	year = {1983},
	pages = {101--105},
	file = {Full Text:/home/grifisaa/Zotero/storage/8GKZ2GSB/Carroll - 1983 - An island parsing interpreter for the full augment.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/SA95XNYC/citation.html:text/html}
}

@techreport{carroll_island_1982,
	title = {An island parsing interpreter for {Augmented} {Transition} {Networks}},
	url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-33.html},
	language = {en},
	number = {UCAM-CL-TR-33},
	urldate = {2019-02-03},
	institution = {University of Cambridge, Computer Laboratory},
	author = {Carroll, John A.},
	year = {1982},
	file = {Full Text PDF:/home/grifisaa/Zotero/storage/NZ5MP4I9/Carroll - 1982 - An island parsing interpreter for Augmented Transi.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/EVDPU549/UCAM-CL-TR-33.html:text/html}
}

@inproceedings{lischka_virtual_2009,
	title = {A virtual network mapping algorithm based on subgraph isomorphism detection},
	booktitle = {Proceedings of the 1st {ACM} workshop on {Virtualized} infrastructure systems and architectures},
	publisher = {ACM},
	author = {Lischka, Jens and Karl, Holger},
	year = {2009},
	pages = {81--88},
	file = {Full Text:/home/grifisaa/Zotero/storage/8WZMGX2N/Lischka and Karl - 2009 - A virtual network mapping algorithm based on subgr.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/G2MDMCZ3/citation.html:text/html}
}

@book{garey_computers_1979,
	series = {Series of books in the mathematical sciences},
	title = {Computers and intractability: {A}  guide to the theory of {NP}-completeness},
	isbn = {978-0-7167-1045-5},
	shorttitle = {Computers and intractability},
	abstract = {Summary: "Shows how to recognize NP-complete problems and offers proactical suggestions for dealing with them effectively. The book covers the basic theory of NP-completeness, provides an overview of alternative directions for further research, and contains and extensive list of NP-complete and NP-hard problems, with more than 300 main entries and several times as many results in total. [This book] is suitable as a supplement to courses in algorithm design, computational complexity, operations research, or combinatorial mathematics, and as a text for seminars on approximation algorithms or computational complexity. It provides not only a valuable source of information for students but also an essential reference work for professionals in computer science"--Back cover.},
	publisher = {W.H. Freeman},
	author = {Garey, Michael R. and Johnson, David S.},
	year = {1979},
	keywords = {Computer programming, Computational complexity, Computer algorithms, Mathematik}
}

@article{mushtaq_multilingual_2017,
	title = {Multilingual {Source} {Code} {Analysis}: {A} {Systematic} {Literature} {Review}},
	volume = {5},
	issn = {2169-3536},
	shorttitle = {Multilingual {Source} {Code} {Analysis}},
	doi = {10.1109/ACCESS.2017.2710421},
	abstract = {Contemporary software applications are developed using cross-language artifacts, which are interdependent with each other. The source code analysis of these applications requires the extraction and examination of artifacts, which are build using multiple programming languages along with their dependencies. A large number of studies presented on multilingual source code analysis and its applications in the last one and half decade. The objective of this systematic literature review (SLR) is to summarize state of the art and prominent areas for future research. This SLR is based on different techniques, tools, and methodologies to analyze multilingual source code applications. We finalized 56 multi-discipline published papers relevant to multilingual source code analysis and its applications out of 3820 papers, filtered through multi-stage search criterion. Based on our findings, we highlight research gaps and challenges in the field of multilingual applications. The research findings are presented in the form of research problems, research contributions, challenges, and future prospects. We identified 46 research issues and requirements for analyzing multilingual applications and grouped them in 13 different software engineering domains. We examined the research contributions and mapped them with individual research problems. We presented the research contributions in the form of tools techniques and approaches that are presented in the form of research models, platforms, frameworks, prototype models, and case studies. Every research has its limitations or prospects for future research. We highlighted the limitations and future perspectives and grouped them in various software engineering domains. Most of the research trends and potential research areas are identified in static source code analysis, program comprehension, refactoring, reverse engineering, detection, and traceability of cross-language links, code coverage, security analysis, cross-language parsing, and abstraction of source code models.},
	journal = {IEEE Access},
	author = {Mushtaq, Z. and Rasool, G. and Shehzad, B.},
	year = {2017},
	note = {bibtex: mushtaqMultilingualSourceCode2017},
	keywords = {Software engineering, reverse engineering, software maintenance, Software, program diagnostics, refactoring, Analytical models, security of data, software design, software architecture, program comprehension, Systematics, Data mining, Reverse engineering, source code (software), Manuals, Bibliographies, contemporary software applications, cross-language artifacts, cross-language parsing, multi-stage search criterion, multilingual applications, multilingual source code analysis, security analysis, source code models, static source code analysis},
	pages = {11307--11336},
	file = {IEEE Xplore Abstract Record:/home/grifisaa/Zotero/storage/HI3KFX6J/7953501.html:text/html;IEEE Xplore Full Text PDF:/home/grifisaa/Zotero/storage/MDSJNQTZ/Mushtaq et al. - 2017 - Multilingual Source Code Analysis A Systematic Li.pdf:application/pdf}
}

@inproceedings{bruneliere_modisco:_2010,
	address = {New York, NY, USA},
	series = {{ASE} '10},
	title = {{MoDisco}: {A} {Generic} and {Extensible} {Framework} for {Model} {Driven} {Reverse} {Engineering}},
	isbn = {978-1-4503-0116-9},
	shorttitle = {{MoDisco}},
	url = {http://doi.acm.org/10.1145/1858996.1859032},
	doi = {10.1145/1858996.1859032},
	abstract = {Nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. Many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. As a consequence, reverse engineering still remains a major challenge for software engineering today. This paper introduces MoDisco, a generic and extensible open source reverse engineering solution. MoDisco intensively uses MDE principles and techniques to improve existing approaches for reverse engineering.},
	urldate = {2019-06-10},
	booktitle = {Proceedings of the {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Bruneliere, Hugo and Cabot, Jordi and Jouault, Frédéric and Madiot, Frédéric},
	year = {2010},
	note = {event-place: Antwerp, Belgium},
	keywords = {reverse engineering, open source, eclipse platform, extensibility, genericity, model driven engineering},
	pages = {173--174},
	file = {Bruneliere et al. - 2010 - MoDisco a generic and extensible framework for mo.pdf:/home/grifisaa/Zotero/storage/PWK3DYHS/Bruneliere et al. - 2010 - MoDisco a generic and extensible framework for mo.pdf:application/pdf}
}

@inproceedings{hunt_efficient_1980,
	address = {New York, NY, USA},
	series = {{POPL} '80},
	title = {Efficient {Algorithms} for {Structural} {Similarity} of {Grammars}},
	isbn = {978-0-89791-011-8},
	url = {http://doi.acm.org/10.1145/567446.567467},
	doi = {10.1145/567446.567467},
	abstract = {Efficient algorithms are presented for several grammar problems relevant to compiler construction. These problems include(i) testing, for a reduced context-free grammar G and an LL(k), uniquely invertible, or BRC(m,n) grammar H, if G is structurally contained by H, and(ii) testing, for a reduced context-free grammar G and a structurally unambiguous grammar H, if G is Reynolds covered by H or if there is an on to homomorphisem from G to H.Related complexity results are presented for several problems for the regular grammars, program schemes, and monadic program schemes.},
	urldate = {2019-06-10},
	booktitle = {Proceedings of the 7th {ACM} {SIGPLAN}-{SIGACT} {Symposium} on {Principles} of {Programming} {Languages}},
	publisher = {ACM},
	author = {Hunt, III, H. B. and Rosenkrantz, D. J.},
	year = {1980},
	note = {event-place: Las Vegas, Nevada},
	pages = {213--219}
}

@article{almeida_solving_2019,
	title = {On solving cycle-free context-free grammar equivalence problem using numerical analysis},
	volume = {51},
	issn = {2590-1184},
	url = {http://www.sciencedirect.com/science/article/pii/S1045926X18301848},
	doi = {10.1016/j.cola.2019.02.005},
	abstract = {In this paper we consider the problem of cycle-free context-free grammars equivalence. To every context-free grammar there corresponds a system of formal equations. Formally applying the iteration method to this system we obtain the grammar axiom in the form of a formal power series composed of the words generated by the grammar ”multiplied” by the respective ambiguities. We define a transform that attributes a matrix meaning to the system of formal equations and to formal power series: terminal symbols are substituted by matrices and formal sum and product are substituted by the matrix ones. In order to effectively compute the sum of a matrix series we numerically solve the system of matrix equations. We prove distinguishability theorems showing that if two formal power series generated by cycle-free context-free grammars are different, then there exists a matrix substitution such that the sums of the respective matrix series are different. Based on this result, we suggest a procedure that can resolve the problem of equivalence of cycle-free context-free grammars in many practical cases. The results obtained in this paper form a theoretical basis for algorithms oriented to automatic assessment of students’ answers in computer science. We present the respective algorithms. Then we compare our approach with a simple heuristic method based on CYK algorithm and discuss the limitations of our method.},
	urldate = {2019-06-10},
	journal = {Journal of Computer Languages},
	author = {Almeida, José João and Grande, Eliana and Smirnov, Georgi},
	month = apr,
	year = {2019},
	keywords = {Automatic assessment, Context-free grammars, Formal languages},
	pages = {48--56},
	file = {illiad.pdf:/home/grifisaa/Zotero/storage/8T5LL8BL/illiad.pdf:application/pdf;ScienceDirect Snapshot:/home/grifisaa/Zotero/storage/AYYH4WDQ/S1045926X18301848.html:text/html}
}

@article{paull_structural_1968,
	title = {Structural equivalence of context-free grammars},
	volume = {2},
	issn = {0022-0000},
	url = {http://www.sciencedirect.com/science/article/pii/S0022000068800376},
	doi = {10.1016/S0022-0000(68)80037-6},
	abstract = {Two context-free grammars are defined as being structurally-equivalent if they generate the same sentences and assign similar parse trees (differing only in the labelling of the nodes) to each. It is argued that this type of equivalence is more significant than weak equivalence, which requires only that the same sentences be generated. While the latter type of equivalence is in general undecidable, it is shown here that there exists a finite algorithm for determining if two arbitrary context-free grammars are structurally equivalent. A related result is a procedure for converting an arbitrary context-free grammar into a structurally equivalent “simple” grammar (S-grammar) where this is possible, or else indicating that no such grammar exists. The question of structural ambiguity is also studied and a procedure is given for determining if an arbitrary context-free grammar can generate the same string in 2 different ways with similar parse trees.},
	number = {4},
	urldate = {2019-06-07},
	journal = {Journal of Computer and System Sciences},
	author = {Paull, Marvin C. and Unger, Stephen H.},
	month = dec,
	year = {1968},
	pages = {427--463},
	file = {ScienceDirect Full Text PDF:/home/grifisaa/Zotero/storage/HPFUAB6F/Paull and Unger - 1968 - Structural equivalence of context-free grammars.pdf:application/pdf;ScienceDirect Snapshot:/home/grifisaa/Zotero/storage/MHERSPCY/S0022000068800376.html:text/html}
}

@article{koch_enumerating_2001,
	title = {Enumerating all connected maximal common subgraphs in two graphs},
	volume = {250},
	number = {1-2},
	journal = {Theoretical Computer Science},
	author = {Koch, Ina},
	year = {2001},
	pages = {1--30},
	file = {Full Text:/home/grifisaa/Zotero/storage/BWJNXAQL/Koch - 2001 - Enumerating all connected maximal common subgraphs.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/9APNUJXD/S0304397500002863.html:text/html}
}

@article{han_frequent_2007,
	title = {Frequent pattern mining: current status and future directions},
	volume = {15},
	shorttitle = {Frequent pattern mining},
	number = {1},
	journal = {Data mining and knowledge discovery},
	author = {Han, Jiawei and Cheng, Hong and Xin, Dong and Yan, Xifeng},
	year = {2007},
	pages = {55--86},
	file = {Full Text:/home/grifisaa/Zotero/storage/B5IU7YY3/s10618-006-0059-1.html:text/html}
}

@article{thomas_margin:_2010,
	title = {Margin: {Maximal} frequent subgraph mining},
	volume = {4},
	shorttitle = {Margin},
	number = {3},
	journal = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
	author = {Thomas, Lini T. and Valluri, Satyanarayana R. and Karlapalem, Kamalakar},
	year = {2010},
	pages = {10},
	file = {Full Text:/home/grifisaa/Zotero/storage/AHYWEBIY/Thomas et al. - 2010 - Margin Maximal frequent subgraph mining.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/UQRARHMA/citation.html:text/html}
}

@inproceedings{lin_large-scale_2014,
	title = {Large-scale frequent subgraph mining in {MapReduce}},
	booktitle = {2014 {IEEE} 30th {International} {Conference} on {Data} {Engineering}},
	publisher = {IEEE},
	author = {Lin, Wenqing and Xiao, Xiaokui and Ghinita, Gabriel},
	year = {2014},
	pages = {844--855},
	file = {Snapshot:/home/grifisaa/Zotero/storage/GZWKI2JT/6816705.html:text/html}
}

@article{koyuturk_efficient_2004,
	title = {An efficient algorithm for detecting frequent subgraphs in biological networks},
	volume = {20},
	number = {suppl\_1},
	journal = {Bioinformatics},
	author = {Koyutürk, Mehmet and Grama, Ananth and Szpankowski, Wojciech},
	year = {2004},
	pages = {i200--i207},
	file = {Full Text:/home/grifisaa/Zotero/storage/CRMJ9TGS/Koyutürk et al. - 2004 - An efficient algorithm for detecting frequent subg.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/3SJ89FYR/217288.html:text/html}
}

@article{chi_mining_2005,
	title = {Mining closed and maximal frequent subtrees from databases of labeled rooted trees},
	volume = {17},
	number = {2},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Chi, Yun and Xia, Yi and Yang, Yirong and Muntz, Richard R.},
	year = {2005},
	pages = {190--202},
	file = {Snapshot:/home/grifisaa/Zotero/storage/RIXAUMEW/1377171.html:text/html}
}

@inproceedings{huan_spin:_2004,
	title = {Spin: mining maximal frequent subgraphs from graph databases},
	shorttitle = {Spin},
	booktitle = {Proceedings of the tenth {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Huan, Jun and Wang, Wei and Prins, Jan and Yang, Jiong},
	year = {2004},
	pages = {581--586},
	file = {Full Text:/home/grifisaa/Zotero/storage/YVIMRLC7/Huan et al. - 2004 - Spin mining maximal frequent subgraphs from graph.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/A446LVIQ/citation.html:text/html}
}

@article{jiang_survey_2013,
	title = {A survey of frequent subgraph mining algorithms},
	volume = {28},
	number = {1},
	journal = {The Knowledge Engineering Review},
	author = {Jiang, Chuntao and Coenen, Frans and Zito, Michele},
	year = {2013},
	pages = {75--105},
	file = {Full Text:/home/grifisaa/Zotero/storage/XGUJQ6QG/Jiang et al. - 2013 - A survey of frequent subgraph mining algorithms.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/7I5MGM86/A58904230A6680001F17FCE91CB8C65F.html:text/html}
}

@incollection{petrinja_how_2013,
	address = {Berlin, Heidelberg},
	title = {How to {Calculate} {Software} {Metrics} for {Multiple} {Languages} {Using} {Open} {Source} {Parsers}},
	volume = {404},
	isbn = {978-3-642-38927-6 978-3-642-38928-3},
	url = {http://link.springer.com/10.1007/978-3-642-38928-3_20},
	abstract = {Source code metrics help to evaluate the quality of the code, for example, to detect the most complex parts of the program. When writing a system which calculates metrics, especially when it has to support multiple source code languages, the biggest problem which arises is the creation of parsers for each supported language. In this paper we suggest an unusual Open Source solution, that avoids creating such parsers from scratch. We suggest and explain how to use parsers contained in the Eclipse IDE as parsers that support contemporary language features, are actively maintained, can recover from errors, and provide not just the abstract syntax tree, but the whole type information of the source program. The ﬁndings described in this paper provide to practitioners a way to use Open Source parsers without the need to deal with parser generators, or to write a parser from scratch.},
	language = {en},
	urldate = {2019-07-22},
	booktitle = {Open {Source} {Software}: {Quality} {Verification}},
	publisher = {Springer Berlin Heidelberg},
	author = {Janes, Andrea and Piatov, Danila and Sillitti, Alberto and Succi, Giancarlo},
	editor = {Petrinja, Etiel and Succi, Giancarlo and El Ioini, Nabil and Sillitti, Alberto},
	year = {2013},
	doi = {10.1007/978-3-642-38928-3_20},
	pages = {264--270},
	file = {Janes et al. - 2013 - How to Calculate Software Metrics for Multiple Lan.pdf:/home/grifisaa/Zotero/storage/GUJXBUJB/Janes et al. - 2013 - How to Calculate Software Metrics for Multiple Lan.pdf:application/pdf}
}

@article{demeyer_famix_2001,
	title = {{FAMIX} 2. 1-the {FAMOOS} information exchange model},
	author = {Demeyer, Serge and Tichelaar, S and Ducasse, Stéphane},
	month = jan,
	year = {2001},
	file = {Full Text PDF:/home/grifisaa/Zotero/storage/H2VPPT5C/Demeyer et al. - 2001 - FAMIX 2. 1-the FAMOOS information exchange model.pdf:application/pdf}
}

@inproceedings{caracciolo_pangea:_2014,
	address = {Victoria, BC, Canada},
	title = {Pangea: {A} {Workbench} for {Statically} {Analyzing} {Multi}-language {Software} {Corpora}},
	isbn = {978-1-4799-6148-1},
	shorttitle = {Pangea},
	url = {http://ieeexplore.ieee.org/document/6975639/},
	doi = {10.1109/SCAM.2014.39},
	abstract = {Software corpora facilitate reproducibility of analyses, however, static analysis for an entire corpus still requires considerable effort, often duplicated unnecessarily by multiple users. Moreover, most corpora are designed for single languages increasing the effort for cross-language analysis. To address these aspects we propose Pangea, an infrastructure allowing fast development of static analyses on multi-language corpora. Pangea uses language-independent meta-models stored as object model snapshots that can be directly loaded into memory and queryed without any parsing overhead. To reduce the effort of performing static analyses, Pangea provides out-of-the box support for: creating and reﬁning analyses in a dedicated environment, deploying an analysis on an entire corpus, using a runner that supports parallel execution, and exporting results in various formats. In this tool demonstration we introduce Pangea and provide several usage scenarios that illustrate how it reduces the cost of analysis.},
	language = {en},
	urldate = {2019-07-22},
	booktitle = {2014 {IEEE} 14th {International} {Working} {Conference} on {Source} {Code} {Analysis} and {Manipulation}},
	publisher = {IEEE},
	author = {Caracciolo, Andrea and Chis, Andrei and Spasojevic, Boris and Lungu, Mircea},
	month = sep,
	year = {2014},
	pages = {71--76},
	file = {Caracciolo et al. - 2014 - Pangea A Workbench for Statically Analyzing Multi.pdf:/home/grifisaa/Zotero/storage/67QS7PWR/Caracciolo et al. - 2014 - Pangea A Workbench for Statically Analyzing Multi.pdf:application/pdf}
}

@incollection{munoz_towards_2011,
	address = {Berlin, Heidelberg},
	title = {Towards the {Detection} of {Cross}-{Language} {Source} {Code} {Reuse}},
	volume = {6716},
	isbn = {978-3-642-22326-6 978-3-642-22327-3},
	url = {http://link.springer.com/10.1007/978-3-642-22327-3_31},
	abstract = {Internet has made available huge amounts of information, also source code. Source code repositories and, in general, programming related websites, facilitate its reuse. In this work, we propose a simple approach to the detection of cross-language source code reuse, a nearly investigated problem. Our preliminary experiments, based on character n-grams comparison, show that considering diﬀerent sections of the code (i.e., comments, code, reserved words, etc.), leads to diﬀerent results. When considering three programming languages: C++, Java, and Python, the best result is obtained when comments are discarded and the entire source code is considered.},
	language = {en},
	urldate = {2019-07-22},
	booktitle = {Natural {Language} {Processing} and {Information} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Flores, Enrique and Barrón-Cedeño, Alberto and Rosso, Paolo and Moreno, Lidia},
	editor = {Muñoz, Rafael and Montoyo, Andrés and Métais, Elisabeth},
	year = {2011},
	doi = {10.1007/978-3-642-22327-3_31},
	pages = {250--253},
	file = {Flores et al. - 2011 - Towards the Detection of Cross-Language Source Cod.pdf:/home/grifisaa/Zotero/storage/YZ6YCR93/Flores et al. - 2011 - Towards the Detection of Cross-Language Source Cod.pdf:application/pdf}
}

@article{van_der_storm_towards_2015,
	series = {Special {Issue} on {New} {Ideas} and {Emerging} {Results} in {Understanding} {Software}},
	title = {Towards multilingual programming environments},
	volume = {97},
	issn = {0167-6423},
	url = {http://www.sciencedirect.com/science/article/pii/S0167642313003341},
	doi = {10.1016/j.scico.2013.11.041},
	abstract = {Software projects consist of different kinds of artifacts: build files, configuration files, markup files, source code in different software languages, and so on. At the same time, however, most integrated development environments (IDEs) are focused on a single (programming) language. Even if a programming environment supports multiple languages (e.g., Eclipse), IDE features such as cross-referencing, refactoring, or debugging, do not often cross language boundaries. What would it mean for programming environment to be truly multilingual? In this short paper we sketch a vision of a system that integrates IDE support across language boundaries. We propose to build this system on a foundation of unified source code models and metaprogramming. Nevertheless, a number of important and hard research questions still need to be addressed.},
	urldate = {2019-07-20},
	journal = {Science of Computer Programming},
	author = {van der Storm, Tijs and Vinju, Jurgen J.},
	month = jan,
	year = {2015},
	keywords = {Language interoperability, Metaprogramming, Programming environments},
	pages = {143--149},
	file = {ScienceDirect Full Text PDF:/home/grifisaa/Zotero/storage/Z9HWAF4Y/van der Storm and Vinju - 2015 - Towards multilingual programming environments.pdf:application/pdf;ScienceDirect Snapshot:/home/grifisaa/Zotero/storage/BNFLLYUZ/S0167642313003341.html:text/html}
}

@inproceedings{offutt_mutation_2006,
	title = {Mutation testing implements grammar-based testing},
	booktitle = {Second {Workshop} on {Mutation} {Analysis} ({Mutation} 2006-{ISSRE} {Workshops} 2006)},
	publisher = {IEEE},
	author = {Offutt, Jeff and Ammann, Paul and Liu, Lisa},
	year = {2006},
	pages = {12--12},
	file = {Offutt et al. - 2006 - Mutation Testing implements Grammar-Based Testing.pdf:/home/grifisaa/Zotero/storage/2TN699FR/Offutt et al. - 2006 - Mutation Testing implements Grammar-Based Testing.pdf:application/pdf;Snapshot:/home/grifisaa/Zotero/storage/CLIKGCQ2/4144731.html:text/html}
}

@inproceedings{offutt_mutation_2006-1,
	title = {Mutation testing implements grammar-based testing},
	doi = {10.1109/MUTATION.2006.11},
	abstract = {This paper presents an abstract view of mutation analysis. Mutation was originally thought of as making changes to program source, but similar kinds of changes have been applied to other artifacts, including program specifications, XML, and input languages. This paper argues that mutation analysis is actually a way to modify any software artifact based on its syntactic description, and is in the same family of test generation methods that create inputs from syntactic descriptions. The essential characteristic of mutation is that a syntactic description such as a grammar is used to create tests. We call this abstract view grammar-based testing, and view it as an interface, which mutation analysis implements. This shift in view allows mutation to be defined in a general way, yielding three benefits. First, it provides a simpler way to understand mutation. Second, it makes it easier to develop future applications of mutation analysis, such as finite state machines and use case collaboration diagrams. The third benefit, which due to space limitations is not explored in this paper, is ensuring that existing techniques are complete according to the criteria defined here.},
	booktitle = {Second {Workshop} on {Mutation} {Analysis} ({Mutation} 2006 - {ISSRE} {Workshops} 2006)},
	author = {Offutt, J. and Ammann, P. and Liu, L.},
	month = nov,
	year = {2006},
	keywords = {Software engineering, Java, program testing, Software testing, Information analysis, mutation testing, grammars, Computer languages, XML, Automata, case collaboration diagrams, Collaboration, finite state machines, Formal specifications, Genetic mutations, grammar-based testing, software artifact},
	pages = {12--12},
	file = {IEEE Xplore Abstract Record:/home/grifisaa/Zotero/storage/KBW7R47D/4144731.html:text/html;IEEE Xplore Full Text PDF:/home/grifisaa/Zotero/storage/JCXJJQFF/Offutt et al. - 2006 - Mutation testing implements grammar-based testing.pdf:application/pdf}
}

@misc{noauthor_antlr/grammars-v4:_nodate,
	title = {antlr/grammars-v4: {Grammars} written for {ANTLR} v4; expectation that the grammars are free of actions.},
	url = {https://github.com/antlr/grammars-v4},
	urldate = {2019-08-01},
	journal = {Github},
	file = {antlr/grammars-v4\: Grammars written for ANTLR v4\; expectation that the grammars are free of actions.:/home/grifisaa/Zotero/storage/82CYMZWT/grammars-v4.html:text/html}
}

@article{chomsky_certain_1959,
	title = {On certain formal properties of grammars},
	volume = {2},
	issn = {0019-9958},
	url = {http://www.sciencedirect.com/science/article/pii/S0019995859903626},
	doi = {10.1016/S0019-9958(59)90362-6},
	abstract = {A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.},
	number = {2},
	urldate = {2019-07-31},
	journal = {Information and Control},
	author = {Chomsky, Noam},
	month = jun,
	year = {1959},
	pages = {137--167},
	file = {ScienceDirect Full Text PDF:/home/grifisaa/Zotero/storage/YVBCRXXC/Chomsky - 1959 - On certain formal properties of grammars.pdf:application/pdf;ScienceDirect Snapshot:/home/grifisaa/Zotero/storage/MPARVU3B/S0019995859903626.html:text/html}
}

@book{cormen_introduction_2001,
	address = {Cambridge Massachusetts},
	edition = {2nd},
	title = {Introduction to {Algorithms}},
	isbn = {0-262-03293-7},
	publisher = {The MIT Press},
	author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford},
	year = {2001}
}

@article{power_metrics_2004,
	title = {A metrics suite for grammar-based software},
	volume = {16},
	issn = {1532-060X, 1532-0618},
	url = {http://doi.wiley.com/10.1002/smr.293},
	doi = {10.1002/smr.293},
	language = {en},
	number = {6},
	urldate = {2019-07-07},
	journal = {Journal of Software Maintenance and Evolution: Research and Practice},
	author = {Power, James F. and Malloy, Brian A.},
	month = nov,
	year = {2004},
	pages = {405--426},
	file = {Submitted Version:/home/grifisaa/Zotero/storage/I3QH28AZ/Power and Malloy - 2004 - A metrics suite for grammar-based software.pdf:application/pdf}
}

@article{kendall_new_1938,
	title = {A {New} {Measure} of {Rank} {Correlation}},
	volume = {30},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2332226?origin=crossref},
	doi = {10.2307/2332226},
	number = {1/2},
	urldate = {2014-06-25},
	journal = {Biometrika},
	author = {Kendall, M. G.},
	month = jun,
	year = {1938},
	pages = {81}
}

@article{anderson_test_1954,
	title = {A {Test} of {Goodness} of {Fit}},
	volume = {49},
	issn = {01621459},
	url = {http://www.jstor.org/stable/2281537?origin=crossref},
	doi = {10.2307/2281537},
	number = {268},
	urldate = {2014-11-11},
	journal = {Journal of the American Statistical Association},
	author = {Anderson, T. W. and Darling, D. A.},
	month = dec,
	year = {1954},
	pages = {765}
}

@article{shapiro_analysis_1965,
	title = {An {Analysis} of {Variance} {Test} for {Normality} ({Complete} {Samples})},
	volume = {52},
	issn = {00063444},
	url = {http://www.jstor.org/stable/2333709?origin=crossref},
	doi = {10.2307/2333709},
	number = {3/4},
	urldate = {2014-10-29},
	journal = {Biometrika},
	author = {Shapiro, S. S. and Wilk, M. B.},
	month = dec,
	year = {1965},
	pages = {591}
}

@article{tukey_comparing_1949,
	title = {Comparing {Individual} {Means} in the {Analysis} of {Variance}},
	volume = {5},
	issn = {0006341X},
	url = {http://www.jstor.org/stable/3001913?origin=crossref},
	doi = {10.2307/3001913},
	number = {2},
	urldate = {2015-09-18},
	journal = {Biometrics},
	author = {Tukey, John W.},
	month = jun,
	year = {1949},
	pages = {99}
}

@inproceedings{van_den_brand_current_1998,
	address = {Ischia, Italy},
	title = {Current parsing techniques in software renovation considered harmful},
	isbn = {978-0-8186-8560-6},
	url = {http://ieeexplore.ieee.org/document/693325/},
	doi = {10.1109/WPC.1998.693325},
	urldate = {2019-07-07},
	booktitle = {Proceedings. 6th {International} {Workshop} on {Program} {Comprehension}. {IWPC}'98 ({Cat}. {No}.98TB100242)},
	publisher = {IEEE Comput. Soc},
	author = {van den Brand, M. and Sellink, A. and Verhoef, C.},
	year = {1998},
	pages = {108--117}
}

@article{csuhaj-varju_descriptional_1993,
	title = {Descriptional complexity of context-free grammar forms},
	volume = {112},
	issn = {03043975},
	url = {https://linkinghub.elsevier.com/retrieve/pii/030439759390021K},
	doi = {10.1016/0304-3975(93)90021-K},
	language = {en},
	number = {2},
	urldate = {2019-07-07},
	journal = {Theoretical Computer Science},
	author = {Csuhaj-Varjú, Erzsébet and Kelemenová, Alica},
	month = may,
	year = {1993},
	pages = {277--289}
}

@book{montgomery_design_2013,
	address = {Hoboken, NJ},
	edition = {Eighth edition},
	title = {Design and analysis of experiments},
	isbn = {978-1-118-14692-7},
	publisher = {John Wiley \& Sons, Inc},
	author = {Montgomery, Douglas C.},
	year = {2013},
	keywords = {Experimental design, TECHNOLOGY \& ENGINEERING / Industrial Engineering}
}

@book{campbell_experimental_1963,
	title = {Experimental and {Quasi}-experimental {Designs} for {Research}},
	publisher = {Rand-McNally},
	author = {Campbell, D. and Stanley, J.},
	year = {1963}
}

@book{wohlin_experimentation_2012,
	address = {Berlin, Heidelberg},
	title = {Experimentation in {Software} {Engineering}},
	isbn = {978-3-642-29043-5 978-3-642-29044-2},
	url = {http://link.springer.com/10.1007/978-3-642-29044-2},
	language = {en},
	urldate = {2014-06-22},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	year = {2012}
}

@inproceedings{dean_grammar_2002,
	address = {Montreal, Que., Canada},
	title = {Grammar programming in {TXL}},
	isbn = {978-0-7695-1793-3},
	url = {http://ieeexplore.ieee.org/document/1134109/},
	doi = {10.1109/SCAM.2002.1134109},
	urldate = {2019-07-07},
	booktitle = {Proceedings. {Second} {IEEE} {International} {Workshop} on {Source} {Code} {Analysis} and {Manipulation}},
	publisher = {IEEE Comput. Soc},
	author = {Dean, T.R. and Cordy, J.R. and Malton, A.J. and Schneider, K.A.},
	year = {2002},
	pages = {93--102}
}

@article{zaytsev_grammar_2015,
	title = {Grammar {Zoo}: {A} corpus of experimental grammarware},
	volume = {98},
	issn = {01676423},
	shorttitle = {Grammar {Zoo}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642314003347},
	doi = {10.1016/j.scico.2014.07.010},
	language = {en},
	urldate = {2019-07-07},
	journal = {Science of Computer Programming},
	author = {Zaytsev, Vadim},
	month = feb,
	year = {2015},
	pages = {28--51}
}

@article{alves_metrication_2005,
	title = {Metrication of {SDF} grammars},
	volume = {1},
	journal = {Dept. de Informática da Univ. do Minho Campus de Gualtar, Braga, Portugal. Rep. Tec. DI-PURe-05.05},
	author = {Alves, Tiago and Visser, Joost},
	year = {2005}
}

@article{anderson_distribution_1962,
	title = {On the {Distribution} of the {Two}-{Sample} {Cramer}-von {Mises} {Criterion}},
	volume = {33},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177704477},
	doi = {10.1214/aoms/1177704477},
	language = {en},
	number = {3},
	urldate = {2014-11-11},
	journal = {The Annals of Mathematical Statistics},
	author = {Anderson, T. W.},
	month = sep,
	year = {1962},
	pages = {1148--1159}
}

@book{campbell_quasi-experimentation:_1979,
	title = {Quasi-experimentation: {Design} and {Analysis} {Issues} for {Field} {Settings}},
	publisher = {Houghton Mifflin Company},
	author = {Campbell, D. and Cook, T. D.},
	year = {1979}
}

@article{ansari_rank-sum_1960,
	title = {Rank-{Sum} {Tests} for {Dispersions}},
	volume = {31},
	issn = {0003-4851},
	url = {http://projecteuclid.org/euclid.aoms/1177705688},
	doi = {10.1214/aoms/1177705688},
	language = {en},
	number = {4},
	urldate = {2014-11-11},
	journal = {The Annals of Mathematical Statistics},
	author = {Ansari, A. R. and Bradley, R. A.},
	month = dec,
	year = {1960},
	pages = {1174--1189}
}

@article{cordy_txl_2004,
	title = {{TXL} - {A} {Language} for {Programming} {Language} {Tools} and {Applications}},
	volume = {110},
	issn = {15710661},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S157106610405217X},
	doi = {10.1016/j.entcs.2004.11.006},
	language = {en},
	urldate = {2019-07-07},
	journal = {Electronic Notes in Theoretical Computer Science},
	author = {Cordy, James R.},
	month = dec,
	year = {2004},
	pages = {3--31},
	file = {Submitted Version:/home/grifisaa/Zotero/storage/2VD6RAX3/Cordy - 2004 - TXL - A Language for Programming Language Tools an.pdf:application/pdf}
}

@article{cordy_txl:_1991,
	title = {{TXL}: {A} rapid prototyping system for programming language dialects},
	volume = {16},
	issn = {00960551},
	shorttitle = {{TXL}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0096055191900196},
	doi = {10.1016/0096-0551(91)90019-6},
	language = {en},
	number = {1},
	urldate = {2019-07-09},
	journal = {Computer Languages},
	author = {Cordy, James R. and Halpern-Hamu, Charles D. and Promislow, Eric},
	month = jan,
	year = {1991},
	pages = {97--107},
	file = {Submitted Version:/home/grifisaa/Zotero/storage/IC5NRLMT/Cordy et al. - 1991 - TXL A rapid prototyping system for programming la.pdf:application/pdf}
}

@book{runeson_case_2012,
	address = {Hoboken, N.J},
	title = {Case study research in software engineering: guidelines and examples},
	isbn = {978-1-118-10435-4},
	shorttitle = {Case study research in software engineering},
	publisher = {Wiley},
	editor = {Runeson, Per},
	year = {2012},
	keywords = {Computer software, Development, COMPUTERS / Software Development \& Engineering / General}
}

@book{yin_case_2009,
	address = {Los Angeles, Calif},
	edition = {4th ed},
	series = {Applied social research methods},
	title = {Case study research: design and methods},
	isbn = {978-1-4129-6099-1},
	shorttitle = {Case study research},
	number = {v. 5},
	publisher = {Sage Publications},
	author = {Yin, Robert K.},
	year = {2009},
	keywords = {Social sciences, Case method, Research Methodology}
}

@book{parr_definitive_2012,
	address = {Dallas, Texas},
	series = {The pragmatic programmers},
	title = {The definitive {ANTLR} 4 reference},
	isbn = {978-1-934356-99-9},
	publisher = {The Pragmatic Bookshelf},
	author = {Parr, Terence},
	year = {2012},
	note = {OCLC: ocn802295434},
	keywords = {Java (Computer program language), Parsing (Computer grammar), Programming languages (Electronic computers), Syntax}
}

@article{kruskal_use_1952,
	title = {Use of {Ranks} in {One}-{Criterion} {Variance} {Analysis}},
	volume = {47},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
	doi = {10.1080/01621459.1952.10483441},
	language = {en},
	number = {260},
	urldate = {2019-08-05},
	journal = {Journal of the American Statistical Association},
	author = {Kruskal, William H. and Wallis, W. Allen},
	month = dec,
	year = {1952},
	pages = {583--621}
}

@techreport{basili_software_1992,
	address = {College Park, MD, USA},
	title = {Software {Modeling} and {Measurement}: {The} {Goal}/{Question}/{Metric} {Paradigm}},
	institution = {University of Maryland at College Park},
	author = {Basili, Victor R.},
	year = {1992}
}

@article{caldiera_goal_1994,
	title = {The goal question metric approach},
	journal = {Encyclopedia of software engineering},
	author = {Caldiera, Victor R Basili1 Gianluigi and Rombach, H Dieter},
	year = {1994},
	pages = {528--532}
}

@inproceedings{coles_pit:_2016,
	address = {Saarbr\&\#252;cken, Germany},
	title = {{PIT}: a practical mutation testing tool for {Java} (demo)},
	isbn = {978-1-4503-4390-9},
	shorttitle = {{PIT}},
	url = {http://dl.acm.org/citation.cfm?doid=2931037.2948707},
	doi = {10.1145/2931037.2948707},
	language = {en},
	urldate = {2019-08-06},
	booktitle = {Proceedings of the 25th {International} {Symposium} on {Software} {Testing} and {Analysis} - {ISSTA} 2016},
	publisher = {ACM Press},
	author = {Coles, Henry and Laurent, Thomas and Henard, Christopher and Papadakis, Mike and Ventresque, Anthony},
	year = {2016},
	pages = {449--452},
	file = {Submitted Version:/home/grifisaa/Zotero/storage/PFDRMK4Y/Coles et al. - 2016 - PIT a practical mutation testing tool for Java (d.pdf:application/pdf}
}

@article{heering_syntax_1989,
	title = {The syntax definition formalism {SDF}---reference manual---},
	volume = {24},
	issn = {03621340},
	url = {http://portal.acm.org/citation.cfm?doid=71605.71607},
	doi = {10.1145/71605.71607},
	language = {en},
	number = {11},
	urldate = {2019-07-09},
	journal = {ACM SIGPLAN Notices},
	author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
	month = nov,
	year = {1989},
	pages = {43--75},
	file = {Submitted Version:/home/grifisaa/Zotero/storage/KG4QBA9S/Heering et al. - 1989 - The syntax definition formalism SDF---reference ma.pdf:application/pdf}
}

@article{box_analysis_1964,
	title = {An {Analysis} of {Transformations}},
	volume = {26},
	copyright = {Copyright © 1964 Royal Statistical Society},
	issn = {00359246},
	url = {http://www.jstor.org/stable/2984418},
	abstract = {In the analysis of data it is often assumed that observations y$_{\textrm{1}}$, y$_{\textrm{2}}$, ..., y$_{\textrm{n}}$ are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
	language = {English},
	number = {2},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Box, G. E. P. and Cox, D. R.},
	year = {1964},
	pages = {pp. 211--252}
}

@article{dunnett_multiple_1955,
	title = {A {Multiple} {Comparison} {Procedure} for {Comparing} {Several} {Treatments} with a {Control}},
	volume = {50},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1955.10501294},
	doi = {10.1080/01621459.1955.10501294},
	language = {en},
	number = {272},
	urldate = {2014-10-29},
	journal = {Journal of the American Statistical Association},
	author = {Dunnett, Charles W.},
	month = dec,
	year = {1955},
	pages = {1096--1121}
}

@article{levene_robust_1960,
	title = {Robust tests for equality of variances1},
	volume = {2},
	journal = {Contributions to probability and statistics: Essays in honor of Harold Hotelling},
	author = {Levene, Howard},
	year = {1960},
	pages = {278--292}
}

@article{rhyne_tables_1965,
	title = {Tables for a {Treatments} versus {Control} {Multiple} {Comparisons} {Sign} {Test}},
	volume = {7},
	issn = {00401706},
	url = {http://www.jstor.org/stable/1266590?origin=crossref},
	doi = {10.2307/1266590},
	number = {3},
	urldate = {2014-12-01},
	journal = {Technometrics},
	author = {Rhyne, A. L. and Steel, R. G. D.},
	month = aug,
	year = {1965},
	pages = {293}
}

@article{steel_multiple_1959,
	title = {A {Multiple} {Comparison} {Rank} {Sum} {Test}: {Treatments} versus {Control}},
	volume = {15},
	issn = {0006341X},
	shorttitle = {A {Multiple} {Comparison} {Rank} {Sum} {Test}},
	url = {http://www.jstor.org/stable/2527654?origin=crossref},
	doi = {10.2307/2527654},
	number = {4},
	urldate = {2014-10-29},
	journal = {Biometrics},
	author = {Steel, Robert G. D.},
	month = dec,
	year = {1959},
	pages = {560}
}

@article{steel_multiple_1959-1,
	title = {A {Multiple} {Comparison} {Sign} {Test}: {Treatments} {Versus} {Control}},
	volume = {54},
	issn = {01621459},
	shorttitle = {A {Multiple} {Comparison} {Sign} {Test}},
	url = {http://www.jstor.org/stable/2282500?origin=crossref},
	doi = {10.2307/2282500},
	number = {288},
	urldate = {2014-12-01},
	journal = {Journal of the American Statistical Association},
	author = {Steel, Robert G. D.},
	month = dec,
	year = {1959},
	pages = {767}
}

@article{steel_rank_1960,
	title = {A {Rank} {Sum} {Test} for {Comparing} {All} {Pairs} of {Treatments}},
	volume = {2},
	issn = {0040-1706, 1537-2723},
	url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1960.10489894},
	doi = {10.1080/00401706.1960.10489894},
	language = {en},
	number = {2},
	urldate = {2014-11-11},
	journal = {Technometrics},
	author = {Steel, Robert G. D.},
	month = may,
	year = {1960},
	pages = {197--207}
}

@article{friedman_use_1937,
	title = {The {Use} of {Ranks} to {Avoid} the {Assumption} of {Normality} {Implicit} in the {Analysis} of {Variance}},
	volume = {32},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1937.10503522},
	doi = {10.1080/01621459.1937.10503522},
	language = {en},
	number = {200},
	urldate = {2019-08-06},
	journal = {Journal of the American Statistical Association},
	author = {Friedman, Milton},
	month = dec,
	year = {1937},
	pages = {675--701}
}

@book{lanza_object-oriented_2011,
	address = {Berlin; London},
	title = {Object-oriented metrics in practice: using software metrics to characterize, evaluate, and improve the design of object-oriented systems},
	isbn = {978-3-642-06374-9},
	shorttitle = {Object-oriented metrics in practice},
	language = {English},
	publisher = {Springer},
	author = {Lanza, Michele and Marinescu, Radu},
	year = {2011},
	note = {OCLC: 750954916}
}