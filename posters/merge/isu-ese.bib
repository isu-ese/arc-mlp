
@book{gareyComputersIntractabilityGuide1979,
  title = {Computers and Intractability: {{A}}  Guide to the Theory of {{NP}}-Completeness},
  isbn = {978-0-7167-1045-5},
  shorttitle = {Computers and Intractability},
  abstract = {Summary: "Shows how to recognize NP-complete problems and offers proactical suggestions for dealing with them effectively. The book covers the basic theory of NP-completeness, provides an overview of alternative directions for further research, and contains and extensive list of NP-complete and NP-hard problems, with more than 300 main entries and several times as many results in total. [This book] is suitable as a supplement to courses in algorithm design, computational complexity, operations research, or combinatorial mathematics, and as a text for seminars on approximation algorithms or computational complexity. It provides not only a valuable source of information for students but also an essential reference work for professionals in computer science"--Back cover.},
  series = {Series of Books in the Mathematical Sciences},
  publisher = {{W.H. Freeman}},
  date = {1979},
  keywords = {Computational complexity,Computer algorithms,Computer programming,Mathematik},
  author = {Garey, Michael R. and Johnson, David S.}
}

@article{mushtaqMultilingualSourceCode2017,
  title = {Multilingual {{Source Code Analysis}}: {{A Systematic Literature Review}}},
  volume = {5},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2017.2710421},
  shorttitle = {Multilingual {{Source Code Analysis}}},
  abstract = {Contemporary software applications are developed using cross-language artifacts, which are interdependent with each other. The source code analysis of these applications requires the extraction and examination of artifacts, which are build using multiple programming languages along with their dependencies. A large number of studies presented on multilingual source code analysis and its applications in the last one and half decade. The objective of this systematic literature review (SLR) is to summarize state of the art and prominent areas for future research. This SLR is based on different techniques, tools, and methodologies to analyze multilingual source code applications. We finalized 56 multi-discipline published papers relevant to multilingual source code analysis and its applications out of 3820 papers, filtered through multi-stage search criterion. Based on our findings, we highlight research gaps and challenges in the field of multilingual applications. The research findings are presented in the form of research problems, research contributions, challenges, and future prospects. We identified 46 research issues and requirements for analyzing multilingual applications and grouped them in 13 different software engineering domains. We examined the research contributions and mapped them with individual research problems. We presented the research contributions in the form of tools techniques and approaches that are presented in the form of research models, platforms, frameworks, prototype models, and case studies. Every research has its limitations or prospects for future research. We highlighted the limitations and future perspectives and grouped them in various software engineering domains. Most of the research trends and potential research areas are identified in static source code analysis, program comprehension, refactoring, reverse engineering, detection, and traceability of cross-language links, code coverage, security analysis, cross-language parsing, and abstraction of source code models.},
  journaltitle = {IEEE Access},
  date = {2017},
  pages = {11307-11336},
  keywords = {Data mining,reverse engineering,Reverse engineering,Analytical models,Bibliographies,contemporary software applications,cross-language artifacts,cross-language parsing,Manuals,multi-stage search criterion,multilingual applications,multilingual source code analysis,program comprehension,program diagnostics,refactoring,security analysis,security of data,Software,software architecture,software design,Software engineering,software maintenance,source code (software),source code models,static source code analysis,Systematics},
  author = {Mushtaq, Z. and Rasool, G. and Shehzad, B.},
  file = {/home/rose/Zotero/storage/MDSJNQTZ/Mushtaq et al. - 2017 - Multilingual Source Code Analysis A Systematic Li.pdf;/home/rose/Zotero/storage/HI3KFX6J/7953501.html}
}

@article{durandEfficientAlgorithmSimilarity1999,
  title = {An Efficient Algorithm for Similarity Analysis of Molecules},
  volume = {2},
  number = {17},
  journaltitle = {Internet Journal of Chemistry},
  date = {1999},
  pages = {1--16},
  author = {Durand, Paul J. and Pasari, Rohit and Baker, Johnnie W. and Tsai, Chun-che},
  file = {/home/rose/Zotero/storage/EP988BBA/paper.html}
}

@inproceedings{wellingPerformanceAnalysisMaximal2011,
  title = {A {{Performance Analysis}} on {{Maximal Common Subgraph Algorithms}}},
  abstract = {Graphs can be used as a tool to determine similarity between structured objects. The maximal common subgraph of two graphs G and H is the largest graph in terms of edges that is isomorphic to a subgraph of G and H. Finding the maximal common subgraph is an NP-complete problem. It is useful in many areas like (bio)chemistry, file versioning and artificial intelligence. There are many papers that evaluate algorithms for finding maximal common induced subgraphs, but little research has been done on the maximal common subgraph that is not an induced subgraph. We have implemented and benchmarked two maximal common (not induced) subgraph algorithms: a backtrack search algorithm (McGregor), and an algorithm that transforms the maximal common subgraph problem to the largest clique problem (Koch). We created generators for randomly connected and mesh structured graphs, these generators have been used to create a database of graph pairs to benchmark the two algorithms. The results of our benchmark have shown that in most cases Koch is more efficient, because after creating the edge product graph needed for the clique detection. The actual clique detection is a relatively simple search.},
  date = {2011},
  keywords = {Artificial intelligence,Backtracking,Benchmark (computing),Clique (graph theory),Clique problem,Graph - visual representation,Induced subgraph,Koch snowflake,Maximal set,Paper,Randomness,Search algorithm},
  author = {Welling, Ruud},
  file = {/home/rose/Zotero/storage/W6DZM6J7/Welling - 2011 - A Performance Analysis on Maximal Common Subgraph .pdf}
}

@article{conteChallengingComplexityMaximum2007,
  title = {Challenging {{Complexity}} of {{Maximum Common Subgraph Detection Algorithms}}: {{A Performance Analysis}} of {{Three Algorithms}} on a {{Wide Database}} of {{Graphs}}},
  volume = {11},
  doi = {10.7155/jgaa.00139},
  shorttitle = {Challenging {{Complexity}} of {{Maximum Common Subgraph Detection Algorithms}}},
  abstract = {Graphs are an extremely general and powerful data structure. In pattern recognition and computer vision, graphs are used to represent patterns to be recognized or classified. Detection of maximum common subgraph (MCS) is useful for matching, comparing and evaluate the similarity of patterns. MCS is a well known NP-complete problem for which optimal and suboptimal algorithms are known from the literature. Nevertheless, until now no effort has been done for characterizing their performance. The lack of a large database of graphs makes the task of comparing the performance of different graph matching algorithms difficult, and often the selection of an algorithm is made on the basis of a few experimental results available. In this paper, three optimal and well-known algorithms for maximum common subgraph detection are described. Moreover a large database containing various categories of pairs of graphs (e.g. random graphs, meshes, bounded valence graphs), is presented, and the performance of the three algorithms is evaluated on this database. Article Type Communicated by Submitted Revised Regular Paper U. Brandes September 2005 January 2007 D. Conte et al., Maximum Common Subgraph, JGAA, 11(1) 99–143 (2007) 100},
  journaltitle = {J. Graph Algorithms Appl.},
  date = {2007},
  pages = {99-143},
  keywords = {Algorithm,Computer vision,Data structure,Graph (discrete mathematics),Journal of Graph Algorithms and Applications,Matching (graph theory),NP-completeness,Pattern recognition,Random graph},
  author = {Conte, Donatello and Foggia, Pasquale and Vento, Mario},
  file = {/home/rose/Zotero/storage/J5G7JFXE/Conte et al. - 2007 - Challenging Complexity of Maximum Common Subgraph .pdf}
}

@inproceedings{conteComparisonThreeMaximum2003,
  location = {{Berlin, Heidelberg}},
  title = {A {{Comparison}} of {{Three Maximum Common Subgraph Algorithms}} on a {{Large Database}} of {{Labeled Graphs}}},
  isbn = {978-3-540-40452-1},
  url = {http://dl.acm.org/citation.cfm?id=1757868.1757884},
  abstract = {A graph g is called a maximum common subgraph of two graphs, g1 and g2, if there exists no other common subgraph of g1 and g2 that has more nodes than g. For the maximum common subgraph problem, exact and inexact algorithms are known from the literature. Nevertheless, until now no effort has been done for characterizing their performance, mainly for the lack of a large database of graphs. In this paper, three exact and well-known algorithms for maximum common subgraph detection are described. Moreover, a large database containing various categories of pairs of graphs (e.g. randomly connected graphs, meshes, bounded valence graphs...), having a maximum common subgraph of at least two nodes, is presented, and the performance of the three algorithms is evaluated on this database.},
  booktitle = {Proceedings of the 4th {{IAPR International Conference}} on {{Graph Based Representations}} in {{Pattern Recognition}}},
  series = {{{GbRPR}}'03},
  publisher = {{Springer-Verlag}},
  urldate = {2019-05-10},
  date = {2003},
  pages = {130--141},
  author = {Conte, D. and Guidobaldi, C. and Sansone, C.},
  file = {/home/rose/Zotero/storage/NM4V4NS3/Conte et al. - 2003 - A Comparison of Three Maximum Common Subgraph Algo.pdf},
  venue = {York, UK}
}

@article{raymondMaximumCommonSubgraph,
  langid = {english},
  title = {Maximum Common Subgraph Isomorphism Algorithms for the Matching of Chemical Structures},
  abstract = {The maximum common subgraph (MCS) problem has become increasingly important in those aspects of chemoinformatics that involve the matching of 2D or 3D chemical structures. This paper provides a classification and a review of the many MCS algorithms, both exact and approximate, that have been described in the literature, and makes recommendations regarding their applicability to typical chemoinformatics tasks.},
  pages = {13},
  author = {Raymond, John W and Willett, Peter},
  file = {/home/rose/Zotero/storage/JFM6B5X6/Raymond and Willett - Maximum common subgraph isomorphism algorithms for.pdf}
}

@article{tomitaWorstcaseTimeComplexity2006,
  title = {The Worst-Case Time Complexity for Generating All Maximal Cliques and Computational Experiments},
  volume = {363},
  issn = {0304-3975},
  url = {http://www.sciencedirect.com/science/article/pii/S0304397506003586},
  doi = {10.1016/j.tcs.2006.06.015},
  abstract = {We present a depth-first search algorithm for generating all maximal cliques of an undirected graph, in which pruning methods are employed as in the Bron–Kerbosch algorithm. All the maximal cliques generated are output in a tree-like form. Subsequently, we prove that its worst-case time complexity is O(3n/3) for an n-vertex graph. This is optimal as a function of n, since there exist up to 3n/3 maximal cliques in an n-vertex graph. The algorithm is also demonstrated to run very fast in practice by computational experiments.},
  number = {1},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  series = {Computing and {{Combinatorics}}},
  urldate = {2019-05-09},
  date = {2006-10-25},
  pages = {28-42},
  keywords = {Computational experiments,Enumeration,Maximal cliques,Worst-case time complexity},
  author = {Tomita, Etsuji and Tanaka, Akira and Takahashi, Haruhisa},
  file = {/home/rose/Zotero/storage/GD8CK37I/Tomita et al. - 2006 - The worst-case time complexity for generating all .pdf;/home/rose/Zotero/storage/DSFQQ4WR/S0304397506003586.html}
}

@inproceedings{bettenburgWhatMakesGood2008,
  title = {What Makes a Good Bug Report?},
  booktitle = {Proceedings of the 16th {{ACM SIGSOFT International Symposium}} on {{Foundations}} of Software Engineering},
  publisher = {{ACM}},
  date = {2008},
  pages = {308--318},
  author = {Bettenburg, Nicolas and Just, Sascha and Schröter, Adrian and Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas},
  file = {/home/rose/Zotero/storage/69P6IZPU/Bettenburg et al. - 2008 - What makes a good bug report.pdf;/home/rose/Zotero/storage/VW9QKCMZ/citation.html}
}

@inproceedings{bastaniSynthesizingProgramInput2017,
  langid = {english},
  location = {{Barcelona, Spain}},
  title = {Synthesizing Program Input Grammars},
  isbn = {978-1-4503-4988-8},
  url = {http://dl.acm.org/citation.cfm?doid=3062341.3062349},
  doi = {10.1145/3062341.3062349},
  abstract = {We present an algorithm for synthesizing a context-free grammar encoding the language of valid program inputs from a set of input examples and blackbox access to the program. Our algorithm addresses shortcomings of existing grammar inference algorithms, which both severely overgeneralize and are prohibitively slow. Our implementation, GLADE, leverages the grammar synthesized by our algorithm to fuzz test programs with structured inputs. We show that GLADE substantially increases the incremental coverage on valid inputs compared to two baseline fuzzers.},
  eventtitle = {The 38th {{ACM SIGPLAN Conference}}},
  booktitle = {Proceedings of the 38th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}  - {{PLDI}} 2017},
  publisher = {{ACM Press}},
  urldate = {2019-04-18},
  date = {2017},
  pages = {95-110},
  author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
  file = {/home/rose/Zotero/storage/RTMDXFT8/Bastani et al. - 2017 - Synthesizing program input grammars.pdf}
}

@article{klintEngineeringDisciplineGrammarware2005,
  langid = {english},
  title = {Toward an Engineering Discipline for Grammarware},
  volume = {14},
  issn = {1049331X},
  url = {http://portal.acm.org/citation.cfm?doid=1072997.1073000},
  doi = {10.1145/1072997.1073000},
  number = {3},
  journaltitle = {ACM Transactions on Software Engineering and Methodology},
  urldate = {2019-04-18},
  date = {2005-07-01},
  pages = {331-380},
  author = {Klint, Paul and Lämmel, Ralf and Verhoef, Chris},
  file = {/home/rose/Zotero/storage/R5CQWU7B/Klint et al. - 2005 - Toward an engineering discipline for grammarware.pdf}
}

@article{goloveshkinTolerantParsingSpecial2018,
  langid = {english},
  title = {Tolerant Parsing with a Special Kind of «{{Any}}» Symbol: The Algorithm and Practical Application},
  volume = {30},
  issn = {20798156, 22206426},
  url = {http://www.ispras.ru/en/proceedings/isp_30_2018_4/isp_30_2018_4_7/},
  doi = {10.15514/ISPRAS-2018-30(4)-1},
  shorttitle = {Tolerant Parsing with a Special Kind of «{{Any}}» Symbol},
  abstract = {Tolerant parsing is a form of syntax analysis aimed at capturing the structure of certain points of interest presented in a source code. While these points should be welldescribed in the corresponding language grammar, other parts of the program are allowed to be not presented in the grammar or to be described coarse-grained, thereby parser remains tolerant to the possible inconsistencies in the irrelevant area. Island grammars are one of the basic tolerant parsing techniques. “Island” is used as the relevant code alias, while the irrelevant code is called “water”. In the paper, a modified LL(1) parsing algorithm with built-in “Any” symbol processing is described. The “Any” symbol matches implicitly defined token sequences. The use of the algorithm for island grammars allows one to reduce irrelevant code description as well as to simplify patterns for relevant code matching. Our “Any” implementation is more accurate and less restrictive in comparison with the closest analogues implemented in Coco/R and LightParse parser generators. It also has potentially lower overhead than the “bounded seas” concept implemented in PetitParser. As shown in the experimental section, the tolerant parser generated by the C\# island grammar is proven to be applicable for large-scale software projects analysis.},
  number = {4},
  journaltitle = {Proceedings of the Institute for System Programming of the RAS},
  urldate = {2019-04-19},
  date = {2018},
  pages = {7-28},
  author = {Goloveshkin, A.V. and Mikhalkovich, S.S.},
  file = {/home/rose/Zotero/storage/BL72465M/SFU, Rostov-on-Don, Russia et al. - 2018 - Tolerant parsing with a special kind of «Any» symb.pdf}
}

@article{shangTamingVerificationHardness2008,
  title = {Taming Verification Hardness: An Efficient Algorithm for Testing Subgraph Isomorphism},
  volume = {1},
  shorttitle = {Taming Verification Hardness},
  number = {1},
  journaltitle = {Proceedings of the VLDB Endowment},
  date = {2008},
  pages = {364--375},
  author = {Shang, Haichuan and Zhang, Ying and Lin, Xuemin and Yu, Jeffrey Xu},
  file = {/home/rose/Zotero/storage/Q3LF2EF4/Shang et al. - 2008 - Taming verification hardness an efficient algorit.pdf;/home/rose/Zotero/storage/3MG7BSLN/citation.html}
}

@inproceedings{synytskyyRobustMultilingualParsing2003,
  title = {Robust Multilingual Parsing Using Island Grammars},
  booktitle = {Proceedings of the 2003 Conference of the {{Centre}} for {{Advanced Studies}} on {{Collaborative}} Research},
  publisher = {{IBM Press}},
  date = {2003},
  pages = {266--278},
  author = {Synytskyy, Nikita and Cordy, James R. and Dean, Thomas R.},
  file = {/home/rose/Zotero/storage/XHC7TP7D/Synytskyy et al. - 2003 - Robust multilingual parsing using island grammars.pdf;/home/rose/Zotero/storage/45F3DFMF/citation.html}
}

@article{grindleyIdentificationTertiaryStructure1993,
  title = {Identification of Tertiary Structure Resemblance in Proteins Using a Maximal Common Subgraph Isomorphism Algorithm},
  volume = {229},
  number = {3},
  journaltitle = {Journal of molecular biology},
  date = {1993},
  pages = {707--721},
  author = {Grindley, Helen M. and Artymiuk, Peter J. and Rice, David W. and Willett, Peter},
  file = {/home/rose/Zotero/storage/C4SEFYTN/S0022283683710740.html}
}

@inproceedings{bacchelliExtractingStructuredData2011,
  title = {Extracting Structured Data from Natural Language Documents with Island Parsing},
  booktitle = {Automated {{Software Engineering}} ({{ASE}}), 2011 26th {{IEEE}}/{{ACM International Conference}} On},
  publisher = {{IEEE}},
  date = {2011},
  pages = {476--479},
  author = {Bacchelli, Alberto and Cleve, Anthony and Lanza, Michele and Mocci, Andrea},
  file = {/home/rose/Zotero/storage/PMUGZSXY/Bacchelli et al. - 2011 - Extracting structured data from natural language d.pdf;/home/rose/Zotero/storage/W9XP9HUD/6100103.html}
}

@inproceedings{streinCrosslanguageProgramAnalysis2006,
  title = {Cross-Language Program Analysis and Refactoring},
  booktitle = {2006 {{Sixth IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}}},
  publisher = {{IEEE}},
  date = {2006},
  pages = {207--216},
  author = {Strein, Dennis and Kratz, Hans and Lowe, Welf},
  file = {/home/rose/Zotero/storage/RBCU5HUQ/Strein et al. - 2006 - Cross-language program analysis and refactoring.pdf;/home/rose/Zotero/storage/NFQSNL6F/4026870.html}
}

@book{reinhardwilhelmCompilerDesign1995,
  location = {{Boston, United States}},
  title = {Compiler {{Design}}},
  isbn = {0-201-42290-5},
  pagetotal = {606},
  publisher = {{Addison-Wesley}},
  date = {1995-01-01},
  author = {Reinhard Wilhelm, Dieter Maurer}
}

@inproceedings{deursenBuildingDocumentationGenerators1999,
  title = {Building Documentation Generators},
  doi = {10.1109/ICSM.1999.792497},
  abstract = {In order to maintain the consistency between sources and documentation, while at the same time providing documentation at the design level, it is necessary to generate documentation from sources in such a way that it can be integrated with hand-written documentation. In order to simplify the construction of documentation generators, we introduce island grammars, which only define those syntactic structures needed for (re)documentation purposes. We explain how they can be used to obtain various forms of documentation, such as data dependency diagrams for mainframe batch jobs. Moreover, we discuss how the derived information can be made available via a hypertext structure. We conclude with an industrial case study in which a 600,000 LOC COBOL legacy system is redocumented using the techniques presented in the paper.},
  eventtitle = {Proceedings {{IEEE International Conference}} on {{Software Maintenance}} - 1999 ({{ICSM}}'99). '{{Software Maintenance}} for {{Business Change}}' ({{Cat}}. {{No}}.{{99CB36360}})},
  booktitle = {Proceedings {{IEEE International Conference}} on {{Software Maintenance}} - 1999 ({{ICSM}}'99). '{{Software Maintenance}} for {{Business Change}}' ({{Cat}}. {{No}}.{{99CB36360}})},
  date = {1999-08},
  pages = {40-49},
  keywords = {600;000 LOC COBOL legacy system,Cost function,data dependency diagrams,design level,Documentation,documentation generators,grammars,hand-written documentation,hypermedia,hypertext structure,island grammars,Lab-on-a-chip,mainframe batch jobs,Outsourcing,Read only memory,redocumentation,syntactic structures,system documentation},
  author = {Deursen, A. Van and Kuipers, T.},
  file = {/home/rose/Zotero/storage/R2BMQH8K/Deursen and Kuipers - 1999 - Building documentation generators.pdf;/home/rose/Zotero/storage/YER64GRN/792497.html}
}

@article{ullmannAlgorithmSubgraphIsomorphism1976,
  title = {An Algorithm for Subgraph Isomorphism},
  volume = {23},
  number = {1},
  journaltitle = {Journal of the ACM (JACM)},
  date = {1976},
  pages = {31--42},
  author = {Ullmann, Julian R.},
  file = {/home/rose/Zotero/storage/NLZT4HNG/Ullmann - 1976 - An algorithm for subgraph isomorphism.pdf;/home/rose/Zotero/storage/XFGZ3G6Q/citation.html}
}

@article{messmerNewAlgorithmErrortolerant1998,
  title = {A New Algorithm for Error-Tolerant Subgraph Isomorphism Detection},
  volume = {20},
  number = {5},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  date = {1998},
  pages = {493--504},
  author = {Messmer, Bruno T. and Bunke, Horst},
  file = {/home/rose/Zotero/storage/TK9VC4P2/Messmer and Bunke - 1998 - A new algorithm for error-tolerant subgraph isomor.pdf;/home/rose/Zotero/storage/B532ITRU/682179.html}
}

@inproceedings{bunkeComparisonAlgorithmsMaximum2002,
  title = {A Comparison of Algorithms for Maximum Common Subgraph on Randomly Connected Graphs},
  booktitle = {Joint {{IAPR International Workshops}} on {{Statistical Techniques}} in {{Pattern Recognition}} ({{SPR}}) and {{Structural}} and {{Syntactic Pattern Recognition}} ({{SSPR}})},
  publisher = {{Springer}},
  date = {2002},
  pages = {123--132},
  author = {Bunke, Horst and Foggia, Pasquale and Guidobaldi, Corrado and Sansone, Carlo and Vento, Mario},
  file = {/home/rose/Zotero/storage/CUCLS6S4/Bunke et al. - 2002 - A comparison of algorithms for maximum common subg.pdf;/home/rose/Zotero/storage/HA46DUSM/10.html}
}

@article{cordellaSubGraphIsomorphism2004,
  title = {A (Sub) Graph Isomorphism Algorithm for Matching Large Graphs},
  volume = {26},
  number = {10},
  journaltitle = {IEEE transactions on pattern analysis and machine intelligence},
  date = {2004},
  pages = {1367--1372},
  author = {Cordella, Luigi P. and Foggia, Pasquale and Sansone, Carlo and Vento, Mario},
  file = {/home/rose/Zotero/storage/IVLLX2RR/Cordella et al. - 2004 - A (sub) graph isomorphism algorithm for matching l.pdf;/home/rose/Zotero/storage/K2NFRBUD/1323804.html}
}

@incollection{eppsteinSubgraphIsomorphismPlanar2002,
  title = {Subgraph Isomorphism in Planar Graphs and Related Problems},
  booktitle = {Graph {{Algorithms And Applications I}}},
  publisher = {{World Scientific}},
  date = {2002},
  pages = {283--309},
  author = {Eppstein, David},
  file = {/home/rose/Zotero/storage/HD5NN6I4/Eppstein - 2002 - Subgraph isomorphism in planar graphs and related .pdf}
}

@inproceedings{moonenLightweightImpactAnalysis2002,
  title = {Lightweight {{Impact Analysis}} Using {{Island Grammars}}.},
  booktitle = {{{IWPC}}},
  publisher = {{Citeseer}},
  date = {2002},
  pages = {219--228},
  author = {Moonen, Leon},
  file = {/home/rose/Zotero/storage/RTH5Q3U9/Moonen - 2002 - Lightweight Impact Analysis using Island Grammars..pdf}
}

@book{haoxiangLanguagesMachinesIntroduction1988,
  langid = {english},
  location = {{Boston, MA, USA}},
  title = {Languages and {{Machines}}: {{An Introduction}} to the {{Theory}} of {{Computer Science}}},
  edition = {3rd},
  isbn = {0-201-15768-3},
  abstract = {Preface The objective of the third edition of Languages and Machines: An Introduction to the Theory of Computer Science remains the same as that of the first two editions, to provide a mathematically sound presentation of the theory of computer},
  publisher = {{Addison-Wesley Longman Publishing Co. Inc.}},
  date = {1988},
  author = {Haoxiang, Ma},
  file = {/home/rose/Zotero/storage/43FAYXNN/Languages_and_Machines_An_Introduction_to_the_Theory_of_Computer_Science.html}
}

@inproceedings{moonenGeneratingRobustParsers2001,
  title = {Generating Robust Parsers Using Island Grammars},
  doi = {10.1109/WCRE.2001.957806},
  abstract = {Source model extraction, the automated extraction of information from system artifacts, is a common phase in reverse engineering tools. One of the major challenges of this phase is creating extractors that can deal with irregularities in the artifacts that are typical for the reverse engineering domain (for example, syntactic errors, incomplete source code, language dialects and embedded languages). The paper proposes a solution in the form of island grammars, a special kind of grammar that combines the detailed specification possibilities of grammars with the liberal behavior of lexical approaches. We show how island grammars can be used to generate robust parsers that combine the accuracy of syntactical analysis with the speed, flexibility and tolerance usually only found in lexical analysis. We conclude with a discussion of the development of MANGROVE, a generator for source model extractors based on island grammars and describe its application to a number of case studies.},
  eventtitle = {Proceedings {{Eighth Working Conference}} on {{Reverse Engineering}}},
  booktitle = {Proceedings {{Eighth Working Conference}} on {{Reverse Engineering}}},
  date = {2001-10},
  pages = {13-22},
  keywords = {grammars,island grammars,Application software,automated information extraction,case studies,computational linguistics,Computer languages,Data mining,detailed specification,embedded languages,fuzzy parsing,incomplete source code,language dialects,lexical approaches,Libraries,Maintenance engineering,MANGROVE,Mars,parser generation,partial parsing,program analysis,program compilers,reverse engineering,Reverse engineering,reverse engineering domain,reverse engineering tools,robust parser generation,robust parsers,Robustness,Software maintenance,source model extraction,source model extractors,syntactic errors,syntactical analysis,system artifacts,Transaction databases},
  author = {Moonen, L.},
  file = {/home/rose/Zotero/storage/HVVDIHDJ/Moonen - 2001 - Generating robust parsers using island grammars.pdf;/home/rose/Zotero/storage/F767EEMY/957806.html}
}

@book{ghezziFundamentalsSoftwareEngineering2002,
  location = {{Upper Saddle River, NJ, USA}},
  title = {Fundamentals of {{Software Engineering}}},
  edition = {2nd},
  isbn = {978-0-13-305699-0},
  abstract = {From the Publisher:This book provides selective, in-depth coverage of the fundamentals of software engineering by stressing principles and methods through rigorous formal and informal approaches. In contrast to other books which are based on the lifecycle model of software development, the authors emphasize identifying and applying fundamental principles that are applicable throughout the software lifecycle. This emphasis enables readers to respond to the rapid changes in technology that are common today. Principles and techniques are emphasized rather than specific tools—users learn why particular techniques should or should not be used. Understanding the principles and techniques on which tools are based makes mastering a variety of specific tools easier. The authors discuss principles such as design, specification, verification, production, management and tools. Now coverage includes: more detailed analysis and explanation of object-oriented techniques; the use of Unified Modeling Language (UML); requirements analysis and software architecture; Model checking—a technique that provides automatic support to the human activity of software verification; GQM—used to evaluate software quality and help improve the software process; Z specification language. For software engineers.},
  publisher = {{Prentice Hall PTR}},
  date = {2002},
  author = {Ghezzi, Carlo and Jazayeri, Mehdi and Mandrioli, Dino}
}

@inproceedings{kuramochiFrequentSubgraphDiscovery2001,
  title = {Frequent Subgraph Discovery},
  booktitle = {Data {{Mining}}, 2001. {{ICDM}} 2001, {{Proceedings IEEE}} International Conference On},
  publisher = {{IEEE}},
  date = {2001},
  pages = {313--320},
  author = {Kuramochi, Michihiro and Karypis, George},
  file = {/home/rose/Zotero/storage/FRLU9FTD/Kuramochi and Karypis - 2001 - Frequent subgraph discovery.pdf;/home/rose/Zotero/storage/TULKIHWA/989534.html}
}

@inproceedings{huanEfficientMiningFrequent2003,
  title = {Efficient Mining of Frequent Subgraphs in the Presence of Isomorphism},
  booktitle = {Null},
  publisher = {{IEEE}},
  date = {2003},
  pages = {549},
  author = {Huan, Jun and Wang, Wei and Prins, Jan},
  file = {/home/rose/Zotero/storage/WINPSKMB/Huan et al. - 2003 - Efficient mining of frequent subgraphs in the pres.pdf;/home/rose/Zotero/storage/L4YQ8NQI/19780549.html}
}

@inproceedings{klusenerDerivingTolerantGrammars2003,
  title = {Deriving Tolerant Grammars from a Base-Line Grammar},
  booktitle = {International {{Conference}} on {{Software Maintenance}}},
  publisher = {{IEEE}},
  date = {2003},
  pages = {179--188},
  author = {Klusener, Steven and Lammel, Ralf},
  file = {/home/rose/Zotero/storage/UFACT5SD/Klusener and Lammel - 2003 - Deriving tolerant grammars from a base-line gramma.pdf;/home/rose/Zotero/storage/NY4WTEIW/1235420.html}
}

@article{kursBoundedSeas2015,
  title = {Bounded Seas},
  volume = {44},
  journaltitle = {Computer languages, systems \& structures},
  date = {2015},
  pages = {114--140},
  author = {Kurš, Jan and Lungu, Mircea and Iyadurai, Rathesan and Nierstrasz, Oscar},
  file = {/home/rose/Zotero/storage/RTWVIN2X/Kurš et al. - 2015 - Bounded seas.pdf;/home/rose/Zotero/storage/2MGNZCVB/S1477842415000536.html}
}

@inproceedings{collardXMLbasedLightweightFact2003,
  title = {An {{XML}}-Based Lightweight {{C}}++ Fact Extractor},
  booktitle = {Program {{Comprehension}}, 2003. 11th {{IEEE International Workshop}} On},
  publisher = {{IEEE}},
  date = {2003},
  pages = {134--143},
  author = {Collard, Michael L. and Kagdi, Huzefa H. and Maletic, Jonathan I.},
  file = {/home/rose/Zotero/storage/VY4I8RQI/Collard et al. - 2003 - An XML-based lightweight C++ fact extractor.pdf;/home/rose/Zotero/storage/2V6ZIF2D/1199197.html}
}

@inproceedings{carrollIslandParsingInterpreter1983,
  title = {An Island Parsing Interpreter for the Full Augmented Transition Network Formalism},
  booktitle = {Proceedings of the First Conference on {{European}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  publisher = {{Association for Computational Linguistics}},
  date = {1983},
  pages = {101--105},
  author = {Carroll, John A.},
  file = {/home/rose/Zotero/storage/8GKZ2GSB/Carroll - 1983 - An island parsing interpreter for the full augment.pdf;/home/rose/Zotero/storage/SA95XNYC/citation.html}
}

@report{carrollIslandParsingInterpreter1982,
  langid = {english},
  title = {An Island Parsing Interpreter for {{Augmented Transition Networks}}},
  url = {https://www.cl.cam.ac.uk/techreports/UCAM-CL-TR-33.html},
  number = {UCAM-CL-TR-33},
  institution = {{University of Cambridge, Computer Laboratory}},
  urldate = {2019-02-03},
  date = {1982},
  author = {Carroll, John A.},
  file = {/home/rose/Zotero/storage/NZ5MP4I9/Carroll - 1982 - An island parsing interpreter for Augmented Transi.pdf;/home/rose/Zotero/storage/EVDPU549/UCAM-CL-TR-33.html}
}

@inproceedings{lischkaVirtualNetworkMapping2009,
  title = {A Virtual Network Mapping Algorithm Based on Subgraph Isomorphism Detection},
  booktitle = {Proceedings of the 1st {{ACM}} Workshop on {{Virtualized}} Infrastructure Systems and Architectures},
  publisher = {{ACM}},
  date = {2009},
  pages = {81--88},
  author = {Lischka, Jens and Karl, Holger},
  file = {/home/rose/Zotero/storage/8WZMGX2N/Lischka and Karl - 2009 - A virtual network mapping algorithm based on subgr.pdf;/home/rose/Zotero/storage/G2MDMCZ3/citation.html}
}

@article{kochEnumeratingAllConnected2001,
  title = {Enumerating All Connected Maximal Common Subgraphs in Two Graphs},
  volume = {250},
  number = {1-2},
  journaltitle = {Theoretical Computer Science},
  date = {2001},
  pages = {1--30},
  author = {Koch, Ina},
  file = {/home/rose/Zotero/storage/BWJNXAQL/Koch - 2001 - Enumerating all connected maximal common subgraphs.pdf;/home/rose/Zotero/storage/9APNUJXD/S0304397500002863.html}
}

@inproceedings{bruneliereMoDiscoGenericExtensible2010,
  location = {{New York, NY, USA}},
  title = {{{MoDisco}}: {{A Generic}} and {{Extensible Framework}} for {{Model Driven Reverse Engineering}}},
  isbn = {978-1-4503-0116-9},
  url = {http://doi.acm.org/10.1145/1858996.1859032},
  doi = {10.1145/1858996.1859032},
  shorttitle = {{{MoDisco}}},
  abstract = {Nowadays, almost all companies, independently of their size and type of activity, are facing the problematic of having to manage, maintain or even replace their legacy systems. Many times, the first problem they need to solve is to really understand what are the functionalities, architecture, data, etc of all these often huge legacy applications. As a consequence, reverse engineering still remains a major challenge for software engineering today. This paper introduces MoDisco, a generic and extensible open source reverse engineering solution. MoDisco intensively uses MDE principles and techniques to improve existing approaches for reverse engineering.},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM International Conference}} on {{Automated Software Engineering}}},
  series = {{{ASE}} '10},
  publisher = {{ACM}},
  urldate = {2019-06-10},
  date = {2010},
  pages = {173--174},
  keywords = {reverse engineering,eclipse platform,extensibility,genericity,model driven engineering,open source},
  author = {Bruneliere, Hugo and Cabot, Jordi and Jouault, Frédéric and Madiot, Frédéric},
  file = {/home/rose/Zotero/storage/PWK3DYHS/Bruneliere et al. - 2010 - MoDisco a generic and extensible framework for mo.pdf},
  venue = {Antwerp, Belgium}
}

@inproceedings{huntEfficientAlgorithmsStructural1980,
  location = {{New York, NY, USA}},
  title = {Efficient {{Algorithms}} for {{Structural Similarity}} of {{Grammars}}},
  isbn = {978-0-89791-011-8},
  url = {http://doi.acm.org/10.1145/567446.567467},
  doi = {10.1145/567446.567467},
  abstract = {Efficient algorithms are presented for several grammar problems relevant to compiler construction. These problems include(i) testing, for a reduced context-free grammar G and an LL(k), uniquely invertible, or BRC(m,n) grammar H, if G is structurally contained by H, and(ii) testing, for a reduced context-free grammar G and a structurally unambiguous grammar H, if G is Reynolds covered by H or if there is an on to homomorphisem from G to H.Related complexity results are presented for several problems for the regular grammars, program schemes, and monadic program schemes.},
  booktitle = {Proceedings of the 7th {{ACM SIGPLAN}}-{{SIGACT Symposium}} on {{Principles}} of {{Programming Languages}}},
  series = {{{POPL}} '80},
  publisher = {{ACM}},
  urldate = {2019-06-10},
  date = {1980},
  pages = {213--219},
  author = {Hunt, III, H. B. and Rosenkrantz, D. J.},
  venue = {Las Vegas, Nevada}
}

@article{almeidaSolvingCyclefreeContextfree2019,
  title = {On Solving Cycle-Free Context-Free Grammar Equivalence Problem Using Numerical Analysis},
  volume = {51},
  issn = {2590-1184},
  url = {http://www.sciencedirect.com/science/article/pii/S1045926X18301848},
  doi = {10.1016/j.cola.2019.02.005},
  abstract = {In this paper we consider the problem of cycle-free context-free grammars equivalence. To every context-free grammar there corresponds a system of formal equations. Formally applying the iteration method to this system we obtain the grammar axiom in the form of a formal power series composed of the words generated by the grammar ”multiplied” by the respective ambiguities. We define a transform that attributes a matrix meaning to the system of formal equations and to formal power series: terminal symbols are substituted by matrices and formal sum and product are substituted by the matrix ones. In order to effectively compute the sum of a matrix series we numerically solve the system of matrix equations. We prove distinguishability theorems showing that if two formal power series generated by cycle-free context-free grammars are different, then there exists a matrix substitution such that the sums of the respective matrix series are different. Based on this result, we suggest a procedure that can resolve the problem of equivalence of cycle-free context-free grammars in many practical cases. The results obtained in this paper form a theoretical basis for algorithms oriented to automatic assessment of students’ answers in computer science. We present the respective algorithms. Then we compare our approach with a simple heuristic method based on CYK algorithm and discuss the limitations of our method.},
  journaltitle = {Journal of Computer Languages},
  shortjournal = {Journal of Computer Languages},
  urldate = {2019-06-10},
  date = {2019-04-01},
  pages = {48-56},
  keywords = {Automatic assessment,Context-free grammars,Formal languages},
  author = {Almeida, José João and Grande, Eliana and Smirnov, Georgi},
  file = {/home/rose/Zotero/storage/8T5LL8BL/illiad.pdf;/home/rose/Zotero/storage/AYYH4WDQ/S1045926X18301848.html}
}

@article{paullStructuralEquivalenceContextfree1968,
  title = {Structural Equivalence of Context-Free Grammars},
  volume = {2},
  issn = {0022-0000},
  url = {http://www.sciencedirect.com/science/article/pii/S0022000068800376},
  doi = {10.1016/S0022-0000(68)80037-6},
  abstract = {Two context-free grammars are defined as being structurally-equivalent if they generate the same sentences and assign similar parse trees (differing only in the labelling of the nodes) to each. It is argued that this type of equivalence is more significant than weak equivalence, which requires only that the same sentences be generated. While the latter type of equivalence is in general undecidable, it is shown here that there exists a finite algorithm for determining if two arbitrary context-free grammars are structurally equivalent. A related result is a procedure for converting an arbitrary context-free grammar into a structurally equivalent “simple” grammar (S-grammar) where this is possible, or else indicating that no such grammar exists. The question of structural ambiguity is also studied and a procedure is given for determining if an arbitrary context-free grammar can generate the same string in 2 different ways with similar parse trees.},
  number = {4},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  urldate = {2019-06-07},
  date = {1968-12-01},
  pages = {427-463},
  author = {Paull, Marvin C. and Unger, Stephen H.},
  file = {/home/rose/Zotero/storage/HPFUAB6F/Paull and Unger - 1968 - Structural equivalence of context-free grammars.pdf;/home/rose/Zotero/storage/MHERSPCY/S0022000068800376.html}
}

@article{hanFrequentPatternMining2007,
  title = {Frequent Pattern Mining: Current Status and Future Directions},
  volume = {15},
  shorttitle = {Frequent Pattern Mining},
  number = {1},
  journaltitle = {Data mining and knowledge discovery},
  date = {2007},
  pages = {55--86},
  author = {Han, Jiawei and Cheng, Hong and Xin, Dong and Yan, Xifeng},
  file = {/home/rose/Zotero/storage/B5IU7YY3/s10618-006-0059-1.html}
}

@article{thomasMarginMaximalFrequent2010,
  title = {Margin: {{Maximal}} Frequent Subgraph Mining},
  volume = {4},
  shorttitle = {Margin},
  number = {3},
  journaltitle = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
  date = {2010},
  pages = {10},
  author = {Thomas, Lini T. and Valluri, Satyanarayana R. and Karlapalem, Kamalakar},
  file = {/home/rose/Zotero/storage/AHYWEBIY/Thomas et al. - 2010 - Margin Maximal frequent subgraph mining.pdf;/home/rose/Zotero/storage/UQRARHMA/citation.html}
}

@inproceedings{linLargescaleFrequentSubgraph2014,
  title = {Large-Scale Frequent Subgraph Mining in {{MapReduce}}},
  booktitle = {2014 {{IEEE}} 30th {{International Conference}} on {{Data Engineering}}},
  publisher = {{IEEE}},
  date = {2014},
  pages = {844--855},
  author = {Lin, Wenqing and Xiao, Xiaokui and Ghinita, Gabriel},
  file = {/home/rose/Zotero/storage/GZWKI2JT/6816705.html}
}

@article{koyuturkEfficientAlgorithmDetecting2004,
  title = {An Efficient Algorithm for Detecting Frequent Subgraphs in Biological Networks},
  volume = {20},
  issue = {suppl\_1},
  journaltitle = {Bioinformatics},
  date = {2004},
  pages = {i200--i207},
  author = {Koyutürk, Mehmet and Grama, Ananth and Szpankowski, Wojciech},
  file = {/home/rose/Zotero/storage/CRMJ9TGS/Koyutürk et al. - 2004 - An efficient algorithm for detecting frequent subg.pdf;/home/rose/Zotero/storage/3SJ89FYR/217288.html}
}

@article{chiMiningClosedMaximal2005,
  title = {Mining Closed and Maximal Frequent Subtrees from Databases of Labeled Rooted Trees},
  volume = {17},
  number = {2},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  date = {2005},
  pages = {190--202},
  author = {Chi, Yun and Xia, Yi and Yang, Yirong and Muntz, Richard R.},
  file = {/home/rose/Zotero/storage/RIXAUMEW/1377171.html}
}

@inproceedings{huanSpinMiningMaximal2004,
  title = {Spin: Mining Maximal Frequent Subgraphs from Graph Databases},
  shorttitle = {Spin},
  booktitle = {Proceedings of the Tenth {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining},
  publisher = {{ACM}},
  date = {2004},
  pages = {581--586},
  author = {Huan, Jun and Wang, Wei and Prins, Jan and Yang, Jiong},
  file = {/home/rose/Zotero/storage/YVIMRLC7/Huan et al. - 2004 - Spin mining maximal frequent subgraphs from graph.pdf;/home/rose/Zotero/storage/A446LVIQ/citation.html}
}

@article{jiangSurveyFrequentSubgraph2013,
  title = {A Survey of Frequent Subgraph Mining Algorithms},
  volume = {28},
  number = {1},
  journaltitle = {The Knowledge Engineering Review},
  date = {2013},
  pages = {75--105},
  author = {Jiang, Chuntao and Coenen, Frans and Zito, Michele},
  file = {/home/rose/Zotero/storage/XGUJQ6QG/Jiang et al. - 2013 - A survey of frequent subgraph mining algorithms.pdf;/home/rose/Zotero/storage/7I5MGM86/A58904230A6680001F17FCE91CB8C65F.html}
}

@inproceedings{offuttMutationTestingImplements2006,
  title = {Mutation Testing Implements Grammar-Based Testing},
  booktitle = {Second {{Workshop}} on {{Mutation Analysis}} ({{Mutation}} 2006-{{ISSRE Workshops}} 2006)},
  publisher = {{IEEE}},
  date = {2006},
  pages = {12--12},
  author = {Offutt, Jeff and Ammann, Paul and Liu, Lisa},
  file = {/home/rose/Zotero/storage/2TN699FR/Offutt et al. - 2006 - Mutation Testing implements Grammar-Based Testing.pdf;/home/rose/Zotero/storage/CLIKGCQ2/4144731.html}
}

@article{vanderstormMultilingualProgrammingEnvironments2015,
  title = {Towards Multilingual Programming Environments},
  volume = {97},
  issn = {0167-6423},
  url = {http://www.sciencedirect.com/science/article/pii/S0167642313003341},
  doi = {10.1016/j.scico.2013.11.041},
  abstract = {Software projects consist of different kinds of artifacts: build files, configuration files, markup files, source code in different software languages, and so on. At the same time, however, most integrated development environments (IDEs) are focused on a single (programming) language. Even if a programming environment supports multiple languages (e.g., Eclipse), IDE features such as cross-referencing, refactoring, or debugging, do not often cross language boundaries. What would it mean for programming environment to be truly multilingual? In this short paper we sketch a vision of a system that integrates IDE support across language boundaries. We propose to build this system on a foundation of unified source code models and metaprogramming. Nevertheless, a number of important and hard research questions still need to be addressed.},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  series = {Special {{Issue}} on {{New Ideas}} and {{Emerging Results}} in {{Understanding Software}}},
  urldate = {2019-07-20},
  date = {2015-01-01},
  pages = {143-149},
  keywords = {Language interoperability,Metaprogramming,Programming environments},
  author = {van der Storm, Tijs and Vinju, Jurgen J.},
  options = {useprefix=true},
  file = {/home/rose/Zotero/storage/Z9HWAF4Y/van der Storm and Vinju - 2015 - Towards multilingual programming environments.pdf;/home/rose/Zotero/storage/BNFLLYUZ/S0167642313003341.html}
}

@incollection{floresDetectionCrossLanguageSource2011,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Towards the {{Detection}} of {{Cross}}-{{Language Source Code Reuse}}},
  volume = {6716},
  isbn = {978-3-642-22326-6 978-3-642-22327-3},
  url = {http://link.springer.com/10.1007/978-3-642-22327-3_31},
  abstract = {Internet has made available huge amounts of information, also source code. Source code repositories and, in general, programming related websites, facilitate its reuse. In this work, we propose a simple approach to the detection of cross-language source code reuse, a nearly investigated problem. Our preliminary experiments, based on character n-grams comparison, show that considering different sections of the code (i.e., comments, code, reserved words, etc.), leads to different results. When considering three programming languages: C++, Java, and Python, the best result is obtained when comments are discarded and the entire source code is considered.},
  booktitle = {Natural {{Language Processing}} and {{Information Systems}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-07-22},
  date = {2011},
  pages = {250-253},
  author = {Flores, Enrique and Barrón-Cedeño, Alberto and Rosso, Paolo and Moreno, Lidia},
  editor = {Muñoz, Rafael and Montoyo, Andrés and Métais, Elisabeth},
  file = {/home/rose/Zotero/storage/YZ6YCR93/Flores et al. - 2011 - Towards the Detection of Cross-Language Source Cod.pdf},
  doi = {10.1007/978-3-642-22327-3_31}
}

@inproceedings{caraccioloPangeaWorkbenchStatically2014,
  langid = {english},
  location = {{Victoria, BC, Canada}},
  title = {Pangea: {{A Workbench}} for {{Statically Analyzing Multi}}-Language {{Software Corpora}}},
  isbn = {978-1-4799-6148-1},
  url = {http://ieeexplore.ieee.org/document/6975639/},
  doi = {10.1109/SCAM.2014.39},
  shorttitle = {Pangea},
  abstract = {Software corpora facilitate reproducibility of analyses, however, static analysis for an entire corpus still requires considerable effort, often duplicated unnecessarily by multiple users. Moreover, most corpora are designed for single languages increasing the effort for cross-language analysis. To address these aspects we propose Pangea, an infrastructure allowing fast development of static analyses on multi-language corpora. Pangea uses language-independent meta-models stored as object model snapshots that can be directly loaded into memory and queryed without any parsing overhead. To reduce the effort of performing static analyses, Pangea provides out-of-the box support for: creating and refining analyses in a dedicated environment, deploying an analysis on an entire corpus, using a runner that supports parallel execution, and exporting results in various formats. In this tool demonstration we introduce Pangea and provide several usage scenarios that illustrate how it reduces the cost of analysis.},
  eventtitle = {2014 {{IEEE}} 14th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}} ({{SCAM}})},
  booktitle = {2014 {{IEEE}} 14th {{International Working Conference}} on {{Source Code Analysis}} and {{Manipulation}}},
  publisher = {{IEEE}},
  urldate = {2019-07-22},
  date = {2014-09},
  pages = {71-76},
  author = {Caracciolo, Andrea and Chis, Andrei and Spasojevic, Boris and Lungu, Mircea},
  file = {/home/rose/Zotero/storage/67QS7PWR/Caracciolo et al. - 2014 - Pangea A Workbench for Statically Analyzing Multi.pdf}
}

@article{demeyerFAMIX1theFAMOOS2001,
  title = {{{FAMIX}} 2. 1-the {{FAMOOS}} Information Exchange Model},
  date = {2001-01-01},
  author = {Demeyer, Serge and Tichelaar, S and Ducasse, Stéphane},
  file = {/home/rose/Zotero/storage/H2VPPT5C/Demeyer et al. - 2001 - FAMIX 2. 1-the FAMOOS information exchange model.pdf}
}

@incollection{janesHowCalculateSoftware2013,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {How to {{Calculate Software Metrics}} for {{Multiple Languages Using Open Source Parsers}}},
  volume = {404},
  isbn = {978-3-642-38927-6 978-3-642-38928-3},
  url = {http://link.springer.com/10.1007/978-3-642-38928-3_20},
  abstract = {Source code metrics help to evaluate the quality of the code, for example, to detect the most complex parts of the program. When writing a system which calculates metrics, especially when it has to support multiple source code languages, the biggest problem which arises is the creation of parsers for each supported language. In this paper we suggest an unusual Open Source solution, that avoids creating such parsers from scratch. We suggest and explain how to use parsers contained in the Eclipse IDE as parsers that support contemporary language features, are actively maintained, can recover from errors, and provide not just the abstract syntax tree, but the whole type information of the source program. The findings described in this paper provide to practitioners a way to use Open Source parsers without the need to deal with parser generators, or to write a parser from scratch.},
  booktitle = {Open {{Source Software}}: {{Quality Verification}}},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2019-07-22},
  date = {2013},
  pages = {264-270},
  author = {Janes, Andrea and Piatov, Danila and Sillitti, Alberto and Succi, Giancarlo},
  editor = {Petrinja, Etiel and Succi, Giancarlo and El Ioini, Nabil and Sillitti, Alberto},
  file = {/home/rose/Zotero/storage/GUJXBUJB/Janes et al. - 2013 - How to Calculate Software Metrics for Multiple Lan.pdf},
  doi = {10.1007/978-3-642-38928-3_20}
}

@book{cormenIntroductionAlgorithms2001,
  location = {{Cambridge Massachusetts}},
  title = {Introduction to {{Algorithms}}},
  edition = {2nd},
  isbn = {0-262-03293-7},
  publisher = {{The MIT Press}},
  date = {2001},
  author = {Cormen, Thomas H. and Leiserson, Charles E. and Rivest, Ronald L. and Stein, Clifford}
}

@article{chomskyCertainFormalProperties1959,
  title = {On Certain Formal Properties of Grammars},
  volume = {2},
  issn = {0019-9958},
  url = {http://www.sciencedirect.com/science/article/pii/S0019995859903626},
  doi = {10.1016/S0019-9958(59)90362-6},
  abstract = {A grammar can be regarded as a device that enumerates the sentences of a language. We study a sequence of restrictions that limit grammars first to Turing machines, then to two types of system from which a phrase structure description of the generated language can be drawn, and finally to finite state Markov sources (finite automata). These restrictions are shown to be increasingly heavy in the sense that the languages that can be generated by grammars meeting a given restriction constitute a proper subset of those that can be generated by grammars meeting the preceding restriction. Various formulations of phrase structure description are considered, and the source of their excess generative power over finite state sources is investigated in greater detail.},
  number = {2},
  journaltitle = {Information and Control},
  shortjournal = {Information and Control},
  urldate = {2019-07-31},
  date = {1959-06-01},
  pages = {137-167},
  author = {Chomsky, Noam},
  file = {/home/rose/Zotero/storage/YVBCRXXC/Chomsky - 1959 - On certain formal properties of grammars.pdf;/home/rose/Zotero/storage/MPARVU3B/S0019995859903626.html}
}

@online{AntlrGrammarsv4Grammars,
  title = {Antlr/Grammars-v4: {{Grammars}} Written for {{ANTLR}} v4; Expectation That the Grammars Are Free of Actions.},
  url = {https://github.com/antlr/grammars-v4},
  journaltitle = {Github},
  urldate = {2019-08-01},
  file = {/home/rose/Zotero/storage/82CYMZWT/grammars-v4.html}
}

@inproceedings{offuttMutationTestingImplements2006a,
  title = {Mutation Testing Implements Grammar-Based Testing},
  doi = {10.1109/MUTATION.2006.11},
  abstract = {This paper presents an abstract view of mutation analysis. Mutation was originally thought of as making changes to program source, but similar kinds of changes have been applied to other artifacts, including program specifications, XML, and input languages. This paper argues that mutation analysis is actually a way to modify any software artifact based on its syntactic description, and is in the same family of test generation methods that create inputs from syntactic descriptions. The essential characteristic of mutation is that a syntactic description such as a grammar is used to create tests. We call this abstract view grammar-based testing, and view it as an interface, which mutation analysis implements. This shift in view allows mutation to be defined in a general way, yielding three benefits. First, it provides a simpler way to understand mutation. Second, it makes it easier to develop future applications of mutation analysis, such as finite state machines and use case collaboration diagrams. The third benefit, which due to space limitations is not explored in this paper, is ensuring that existing techniques are complete according to the criteria defined here.},
  eventtitle = {Second {{Workshop}} on {{Mutation Analysis}} ({{Mutation}} 2006 - {{ISSRE Workshops}} 2006)},
  booktitle = {Second {{Workshop}} on {{Mutation Analysis}} ({{Mutation}} 2006 - {{ISSRE Workshops}} 2006)},
  date = {2006-11},
  pages = {12-12},
  keywords = {grammars,Computer languages,Software engineering,Automata,case collaboration diagrams,Collaboration,finite state machines,Formal specifications,Genetic mutations,grammar-based testing,Information analysis,Java,mutation testing,program testing,software artifact,Software testing,XML},
  author = {Offutt, J. and Ammann, P. and Liu, L.},
  file = {/home/rose/Zotero/storage/JCXJJQFF/Offutt et al. - 2006 - Mutation testing implements grammar-based testing.pdf;/home/rose/Zotero/storage/KBW7R47D/4144731.html}
}

@article{friedmanUseRanksAvoid1937,
  langid = {english},
  title = {The Use of Ranks to Avoid the Assumption of Normality Implicit in the Analysis of Variance},
  volume = {32},
  issn = {0162-1459, 1537-274X},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1937.10503522},
  doi = {10.1080/01621459.1937.10503522},
  number = {200},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  urldate = {2019-08-06},
  date = {1937-12},
  pages = {675-701},
  author = {Friedman, Milton}
}

@article{steelRankSumTest1960,
  langid = {english},
  title = {A {{Rank Sum Test}} for {{Comparing All Pairs}} of {{Treatments}}},
  volume = {2},
  issn = {0040-1706, 1537-2723},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00401706.1960.10489894},
  doi = {10.1080/00401706.1960.10489894},
  number = {2},
  journaltitle = {Technometrics},
  urldate = {2014-11-11},
  date = {1960-05},
  pages = {197-207},
  author = {Steel, Robert G. D.}
}

@article{steelMultipleComparisonSign1959,
  eprinttype = {jstor},
  eprint = {2282500?origin=crossref},
  title = {A {{Multiple Comparison Sign Test}}: {{Treatments Versus Control}}},
  volume = {54},
  issn = {01621459},
  doi = {10.2307/2282500},
  shorttitle = {A {{Multiple Comparison Sign Test}}},
  number = {288},
  journaltitle = {Journal of the American Statistical Association},
  date = {1959-12},
  pages = {767},
  author = {Steel, Robert G. D.}
}

@article{dunnettMultipleComparisonProcedure1955,
  langid = {english},
  title = {A {{Multiple Comparison Procedure}} for {{Comparing Several Treatments}} with a {{Control}}},
  volume = {50},
  issn = {0162-1459, 1537-274X},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1955.10501294},
  doi = {10.1080/01621459.1955.10501294},
  number = {272},
  journaltitle = {Journal of the American Statistical Association},
  urldate = {2014-10-29},
  date = {1955-12},
  pages = {1096-1121},
  author = {Dunnett, Charles W.}
}

@article{boxAnalysisTransformations1964,
  eprinttype = {jstor},
  eprint = {2984418},
  langid = {english},
  title = {An {{Analysis}} of {{Transformations}}},
  volume = {26},
  issn = {00359246},
  abstract = {In the analysis of data it is often assumed that observations y\textsubscript{1}, y\textsubscript{2}, ..., y\textsubscript{n} are independently normally distributed with constant variance and with expectations specified by a model linear in a set of parameters θ. In this paper we make the less restrictive assumption that such a normal, homoscedastic, linear model is appropriate after some suitable transformation has been applied to the y's. Inferences about the transformation and about the parameters of the linear model are made by computing the likelihood function and the relevant posterior distribution. The contributions of normality, homoscedasticity and additivity to the transformation are separated. The relation of the present methods to earlier procedures for finding transformations is discussed. The methods are illustrated with examples.},
  number = {2},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  date = {1964},
  pages = {pp. 211-252},
  author = {Box, G. E. P. and Cox, D. R.}
}

@article{steelMultipleComparisonRank1959,
  eprinttype = {jstor},
  eprint = {2527654?origin=crossref},
  title = {A Multiple Comparison Rank Sum Test: {{Treatments}} versus Control},
  volume = {15},
  issn = {0006341X},
  doi = {10.2307/2527654},
  shorttitle = {A {{Multiple Comparison Rank Sum Test}}},
  number = {4},
  journaltitle = {Biometrics},
  date = {1959-12},
  pages = {560},
  author = {Steel, Robert G. D.}
}

@article{rhyneTablesTreatmentsControl1965,
  eprinttype = {jstor},
  eprint = {1266590?origin=crossref},
  title = {Tables for a {{Treatments}} versus {{Control Multiple Comparisons Sign Test}}},
  volume = {7},
  issn = {00401706},
  doi = {10.2307/1266590},
  number = {3},
  journaltitle = {Technometrics},
  date = {1965-08},
  pages = {293},
  author = {Rhyne, A. L. and Steel, R. G. D.}
}

@article{leveneRobustTestsEquality1960,
  title = {Robust Tests for Equality of Variances},
  volume = {2},
  journaltitle = {Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling},
  date = {1960},
  pages = {278--292},
  author = {Levene, Howard}
}

@article{heeringSyntaxDefinitionFormalism1989,
  langid = {english},
  title = {The Syntax Definition Formalism {{SDF}}---Reference Manual---},
  volume = {24},
  issn = {03621340},
  url = {http://portal.acm.org/citation.cfm?doid=71605.71607},
  doi = {10.1145/71605.71607},
  number = {11},
  journaltitle = {ACM SIGPLAN Notices},
  shortjournal = {SIGPLAN Not.},
  urldate = {2019-07-09},
  date = {1989-11-01},
  pages = {43-75},
  author = {Heering, J. and Hendriks, P. R. H. and Klint, P. and Rekers, J.},
  file = {/home/rose/Zotero/storage/KG4QBA9S/Heering et al. - 1989 - The syntax definition formalism SDF---reference ma.pdf}
}

@inproceedings{colesPITPracticalMutation2016,
  langid = {english},
  location = {{Saarbr\&\#252;cken, Germany}},
  title = {{{PIT}}: A Practical Mutation Testing Tool for {{Java}} (Demo)},
  isbn = {978-1-4503-4390-9},
  url = {http://dl.acm.org/citation.cfm?doid=2931037.2948707},
  doi = {10.1145/2931037.2948707},
  shorttitle = {{{PIT}}},
  eventtitle = {The 25th {{International Symposium}}},
  booktitle = {Proceedings of the 25th {{International Symposium}} on {{Software Testing}} and {{Analysis}} - {{ISSTA}} 2016},
  publisher = {{ACM Press}},
  urldate = {2019-08-06},
  date = {2016},
  pages = {449-452},
  author = {Coles, Henry and Laurent, Thomas and Henard, Christopher and Papadakis, Mike and Ventresque, Anthony},
  file = {/home/rose/Zotero/storage/PFDRMK4Y/Coles et al. - 2016 - PIT a practical mutation testing tool for Java (d.pdf}
}

@article{caldieraGoalQuestionMetric1994,
  title = {The Goal Question Metric Approach},
  journaltitle = {Encyclopedia of software engineering},
  date = {1994},
  pages = {528--532},
  author = {Caldiera, Victor R Basili1 Gianluigi and Rombach, H Dieter}
}

@report{basiliSoftwareModelingMeasurement1992,
  location = {{College Park, MD, USA}},
  title = {Software {{Modeling}} and {{Measurement}}: {{The Goal}}/{{Question}}/{{Metric Paradigm}}},
  institution = {{University of Maryland at College Park}},
  date = {1992},
  author = {Basili, Victor R.}
}

@article{kruskalUseRanksOneCriterion1952,
  langid = {english},
  title = {Use of {{Ranks}} in {{One}}-{{Criterion Variance Analysis}}},
  volume = {47},
  issn = {0162-1459, 1537-274X},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
  doi = {10.1080/01621459.1952.10483441},
  number = {260},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  urldate = {2019-08-05},
  date = {1952-12},
  pages = {583-621},
  author = {Kruskal, William H. and Wallis, W. Allen}
}

@book{parrDefinitiveANTLRReference2012,
  location = {{Dallas, Texas}},
  title = {The Definitive {{ANTLR}} 4 Reference},
  isbn = {978-1-934356-99-9},
  pagetotal = {305},
  series = {The Pragmatic Programmers},
  publisher = {{The Pragmatic Bookshelf}},
  date = {2012},
  keywords = {Java (Computer program language),Parsing (Computer grammar),Programming languages (Electronic computers),Syntax},
  author = {Parr, Terence},
  note = {OCLC: ocn802295434}
}

@collection{runesonCaseStudyResearch2012,
  location = {{Hoboken, N.J}},
  title = {Case Study Research in Software Engineering: Guidelines and Examples},
  isbn = {978-1-118-10435-4},
  shorttitle = {Case Study Research in Software Engineering},
  pagetotal = {237},
  publisher = {{Wiley}},
  date = {2012},
  keywords = {Computer software,COMPUTERS / Software Development & Engineering / General,Development},
  editor = {Runeson, Per}
}

@book{yinCaseStudyResearch2009,
  location = {{Los Angeles, Calif}},
  title = {Case Study Research: Design and Methods},
  edition = {4th ed},
  isbn = {978-1-4129-6099-1},
  shorttitle = {Case Study Research},
  pagetotal = {219},
  number = {v. 5},
  series = {Applied Social Research Methods},
  publisher = {{Sage Publications}},
  date = {2009},
  keywords = {Case method,Research Methodology,Social sciences},
  author = {Yin, Robert K.}
}

@article{cordyTXLRapidPrototyping1991,
  langid = {english},
  title = {{{TXL}}: {{A}} Rapid Prototyping System for Programming Language Dialects},
  volume = {16},
  issn = {00960551},
  url = {https://linkinghub.elsevier.com/retrieve/pii/0096055191900196},
  doi = {10.1016/0096-0551(91)90019-6},
  shorttitle = {{{TXL}}},
  number = {1},
  journaltitle = {Computer Languages},
  shortjournal = {Computer Languages},
  urldate = {2019-07-09},
  date = {1991-01},
  pages = {97-107},
  author = {Cordy, James R. and Halpern-Hamu, Charles D. and Promislow, Eric},
  file = {/home/rose/Zotero/storage/IC5NRLMT/Cordy et al. - 1991 - TXL A rapid prototyping system for programming la.pdf}
}

@book{campbellQuasiexperimentationDesignAnalysis1979,
  title = {Quasi-Experimentation: {{Design}} and {{Analysis Issues}} for {{Field Settings}}},
  publisher = {{Houghton Mifflin Company}},
  date = {1979},
  author = {Campbell, D. and Cook, T. D.}
}

@article{zaytsevGrammarZooCorpus2015,
  langid = {english},
  title = {Grammar {{Zoo}}: {{A}} Corpus of Experimental Grammarware},
  volume = {98},
  issn = {01676423},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642314003347},
  doi = {10.1016/j.scico.2014.07.010},
  shorttitle = {Grammar {{Zoo}}},
  journaltitle = {Science of Computer Programming},
  shortjournal = {Science of Computer Programming},
  urldate = {2019-07-07},
  date = {2015-02},
  pages = {28-51},
  author = {Zaytsev, Vadim}
}

@inproceedings{deanGrammarProgrammingTXL2002,
  location = {{Montreal, Que., Canada}},
  title = {Grammar Programming in {{TXL}}},
  isbn = {978-0-7695-1793-3},
  url = {http://ieeexplore.ieee.org/document/1134109/},
  doi = {10.1109/SCAM.2002.1134109},
  eventtitle = {Second {{IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}}},
  booktitle = {Proceedings. {{Second IEEE International Workshop}} on {{Source Code Analysis}} and {{Manipulation}}},
  publisher = {{IEEE Comput. Soc}},
  urldate = {2019-07-07},
  date = {2002},
  pages = {93-102},
  author = {Dean, T.R. and Cordy, J.R. and Malton, A.J. and Schneider, K.A.}
}

@book{campbellExperimentalQuasiexperimentalDesigns1963,
  title = {Experimental and {{Quasi}}-Experimental {{Designs}} for {{Research}}},
  publisher = {{Rand-McNally}},
  date = {1963},
  author = {Campbell, D. and Stanley, J.}
}

@book{montgomeryDesignAnalysisExperiments2013,
  location = {{Hoboken, NJ}},
  title = {Design and Analysis of Experiments},
  edition = {Eighth edition},
  isbn = {978-1-118-14692-7},
  pagetotal = {730},
  publisher = {{John Wiley \& Sons, Inc}},
  date = {2013},
  keywords = {Experimental design,TECHNOLOGY & ENGINEERING / Industrial Engineering},
  author = {Montgomery, Douglas C.}
}

@article{csuhaj-varjuDescriptionalComplexityContextfree1993,
  langid = {english},
  title = {Descriptional Complexity of Context-Free Grammar Forms},
  volume = {112},
  issn = {03043975},
  url = {https://linkinghub.elsevier.com/retrieve/pii/030439759390021K},
  doi = {10.1016/0304-3975(93)90021-K},
  number = {2},
  journaltitle = {Theoretical Computer Science},
  shortjournal = {Theoretical Computer Science},
  urldate = {2019-07-07},
  date = {1993-05},
  pages = {277-289},
  author = {Csuhaj-Varjú, Erzsébet and Kelemenová, Alica}
}

@inproceedings{vandenbrandCurrentParsingTechniques1998,
  location = {{Ischia, Italy}},
  title = {Current Parsing Techniques in Software Renovation Considered Harmful},
  isbn = {978-0-8186-8560-6},
  url = {http://ieeexplore.ieee.org/document/693325/},
  doi = {10.1109/WPC.1998.693325},
  eventtitle = {6th {{International Workshop}} on {{Program Comprehension}}. {{IWPC}}'98},
  booktitle = {Proceedings. 6th {{International Workshop}} on {{Program Comprehension}}. {{IWPC}}'98 ({{Cat}}. {{No}}.{{98TB100242}})},
  publisher = {{IEEE Comput. Soc}},
  urldate = {2019-07-07},
  date = {1998},
  pages = {108-117},
  author = {van den Brand, M. and Sellink, A. and Verhoef, C.},
  options = {useprefix=true}
}

@article{shapiroAnalysisVarianceTest1965,
  eprinttype = {jstor},
  eprint = {2333709?origin=crossref},
  title = {An {{Analysis}} of {{Variance Test}} for {{Normality}} ({{Complete Samples}})},
  volume = {52},
  issn = {00063444},
  doi = {10.2307/2333709},
  number = {3/4},
  journaltitle = {Biometrika},
  date = {1965-12},
  pages = {591},
  author = {Shapiro, S. S. and Wilk, M. B.}
}

@article{cordyTXLLanguageProgramming2004,
  langid = {english},
  title = {{{TXL}} - {{A Language}} for {{Programming Language Tools}} and {{Applications}}},
  volume = {110},
  issn = {15710661},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S157106610405217X},
  doi = {10.1016/j.entcs.2004.11.006},
  journaltitle = {Electronic Notes in Theoretical Computer Science},
  shortjournal = {Electronic Notes in Theoretical Computer Science},
  urldate = {2019-07-07},
  date = {2004-12},
  pages = {3-31},
  author = {Cordy, James R.},
  file = {/home/rose/Zotero/storage/2VD6RAX3/Cordy - 2004 - TXL - A Language for Programming Language Tools an.pdf}
}

@article{ansariRankSumTestsDispersions1960,
  langid = {english},
  title = {Rank-{{Sum Tests}} for {{Dispersions}}},
  volume = {31},
  issn = {0003-4851},
  url = {http://projecteuclid.org/euclid.aoms/1177705688},
  doi = {10.1214/aoms/1177705688},
  number = {4},
  journaltitle = {The Annals of Mathematical Statistics},
  urldate = {2014-11-11},
  date = {1960-12},
  pages = {1174-1189},
  author = {Ansari, A. R. and Bradley, R. A.}
}

@article{andersonDistributionTwoSampleCramervon1962,
  langid = {english},
  title = {On the {{Distribution}} of the {{Two}}-{{Sample Cramer}}-von {{Mises Criterion}}},
  volume = {33},
  issn = {0003-4851},
  url = {http://projecteuclid.org/euclid.aoms/1177704477},
  doi = {10.1214/aoms/1177704477},
  number = {3},
  journaltitle = {The Annals of Mathematical Statistics},
  urldate = {2014-11-11},
  date = {1962-09},
  pages = {1148-1159},
  author = {Anderson, T. W.}
}

@article{alvesMetricationSDFGrammars2005,
  title = {Metrication of {{SDF}} Grammars},
  volume = {1},
  journaltitle = {Dept. de Informática da Univ. do Minho Campus de Gualtar, Braga, Portugal. Rep. Tec. DI-PURe-05.05},
  date = {2005},
  author = {Alves, Tiago and Visser, Joost}
}

@book{wohlinExperimentationSoftwareEngineering2012,
  langid = {english},
  location = {{Berlin, Heidelberg}},
  title = {Experimentation in {{Software Engineering}}},
  isbn = {978-3-642-29043-5 978-3-642-29044-2},
  url = {http://link.springer.com/10.1007/978-3-642-29044-2},
  publisher = {{Springer Berlin Heidelberg}},
  urldate = {2014-06-22},
  date = {2012},
  author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders}
}

@article{tukeyComparingIndividualMeans1949,
  eprinttype = {jstor},
  eprint = {3001913?origin=crossref},
  title = {Comparing {{Individual Means}} in the {{Analysis}} of {{Variance}}},
  volume = {5},
  issn = {0006341X},
  doi = {10.2307/3001913},
  number = {2},
  journaltitle = {Biometrics},
  date = {1949-06},
  pages = {99},
  author = {Tukey, John W.}
}

@article{andersonTestGoodnessFit1954,
  eprinttype = {jstor},
  eprint = {2281537?origin=crossref},
  title = {A Test of Goodness of Fit},
  volume = {49},
  issn = {01621459},
  doi = {10.2307/2281537},
  number = {268},
  journaltitle = {Journal of the American Statistical Association},
  date = {1954-12},
  pages = {765},
  author = {Anderson, T. W. and Darling, D. A.}
}

@article{kendallNewMeasureRank1938,
  eprinttype = {jstor},
  eprint = {2332226?origin=crossref},
  title = {A {{New Measure}} of {{Rank Correlation}}},
  volume = {30},
  issn = {00063444},
  doi = {10.2307/2332226},
  number = {1/2},
  journaltitle = {Biometrika},
  date = {1938-06},
  pages = {81},
  author = {Kendall, M. G.}
}

@article{powerMetricsSuiteGrammarbased2004,
  langid = {english},
  title = {A Metrics Suite for Grammar-Based Software},
  volume = {16},
  issn = {1532-060X, 1532-0618},
  url = {http://doi.wiley.com/10.1002/smr.293},
  doi = {10.1002/smr.293},
  number = {6},
  journaltitle = {Journal of Software Maintenance and Evolution: Research and Practice},
  shortjournal = {J. Softw. Maint. Evol.: Res. Pract.},
  urldate = {2019-07-07},
  date = {2004-11},
  pages = {405-426},
  author = {Power, James F. and Malloy, Brian A.},
  file = {/home/rose/Zotero/storage/I3QH28AZ/Power and Malloy - 2004 - A metrics suite for grammar-based software.pdf}
}

@book{lanzaObjectorientedMetricsPractice2011,
  langid = {english},
  location = {{Berlin; London}},
  title = {Object-Oriented Metrics in Practice: Using Software Metrics to Characterize, Evaluate, and Improve the Design of Object-Oriented Systems},
  isbn = {978-3-642-06374-9},
  shorttitle = {Object-Oriented Metrics in Practice},
  publisher = {{Springer}},
  date = {2011},
  author = {Lanza, Michele and Marinescu, Radu},
  note = {OCLC: 750954916}
}

@article{rosenkrantzMatrixEquationsNormal1967,
  title = {Matrix {{Equations}} and {{Normal Forms}} for {{Context}}-{{Free Grammars}}},
  volume = {14},
  issn = {0004-5411},
  url = {http://doi.acm.org/10.1145/321406.321412},
  doi = {10.1145/321406.321412},
  abstract = {The relationship between the set of productions of a context-free grammar and the corresponding set of defining equations is first pointed out. The closure operation on a matrix of strings is defined and this concept is used to formalize the solution to a set of linear equations. A procedure is then given for rewriting a context-free grammar in Greibach normal form, where the replacements string of each production begins with a terminal symbol. An additional procedure is given for rewriting the grammar so that each replacement string both begins and ends with a terminal symbol. Neither procedure requires the evaluation of regular begins and ends with a terminal symbol. Neither procedure requires the evaluation of regular expressions over the total vocabulary of the grammar, as is required by Greibach's procedure.},
  number = {3},
  journaltitle = {J. ACM},
  urldate = {2019-09-25},
  date = {1967-07},
  pages = {501--507},
  author = {Rosenkrantz, Daniel J.},
  file = {/home/rose/Zotero/storage/MYCKAIMM/Rosenkrantz - 1967 - Matrix Equations and Normal Forms for Context-Free.pdf}
}

@book{higginsIntroductionModernNonparametric2004,
  location = {{Pacific Grove, CA}},
  title = {An Introduction to Modern Nonparametric Statistics},
  isbn = {978-0-534-38775-4},
  pagetotal = {366},
  publisher = {{Brooks/Cole}},
  date = {2004},
  keywords = {Nonparametric statistics},
  author = {Higgins, James J.}
}

@article{jonckheereDistributionFreeKSampleTest1954,
  title = {A {{Distribution}}-{{Free}} k-{{Sample Test Against Ordered Alternatives}}},
  volume = {41},
  issn = {00063444},
  url = {https://www.jstor.org/stable/2333011?origin=crossref},
  doi = {10.2307/2333011},
  number = {1/2},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  urldate = {2019-09-30},
  date = {1954-06},
  pages = {133},
  author = {Jonckheere, A. R.}
}


